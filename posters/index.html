<!DOCTYPE html>
<html lang="en-US">

<!-- Mirrored from ieeevr.org/2016/posters/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 22 May 2020 15:39:14 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
		<meta charset="UTF-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<link rel="profile" href="http://gmpg.org/xfn/11" />
	<link rel="pingback" href="#" />
	<title>Posters | IEEE VR 2016</title>
<link rel="alternate" type="application/rss+xml" title="IEEE VR 2016 &raquo; Feed" href="../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="IEEE VR 2016 &raquo; Comments Feed" href="../comments/feed/index.html" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"http:\/\/s.w.org\/images\/core\/emoji\/72x72\/","ext":".png","source":{"concatemoji":"http:\/\/ieeevr.org\/2016\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.2.2"}};
			!function(a,b,c){function d(a){var c=b.createElement("canvas"),d=c.getContext&&c.getContext("2d");return d&&d.fillText?(d.textBaseline="top",d.font="600 32px Arial","flag"===a?(d.fillText(String.fromCharCode(55356,56812,55356,56807),0,0),c.toDataURL().length>3e3):(d.fillText(String.fromCharCode(55357,56835),0,0),0!==d.getImageData(16,16,1,1).data[0])):!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g;c.supports={simple:d("simple"),flag:d("flag")},c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.simple&&c.supports.flag||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='SFSImainCss-css' href='../wp-content/plugins/ultimate-social-media-icons/css/sfsi-style3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='SFSIJqueryCSS-css' href='../wp-content/plugins/ultimate-social-media-icons/css/jquery-ui-1.10.4/jquery-ui-min3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='flat-fonts-css' href='http://fonts.googleapis.com/css?family=Amatic+SC%7CRoboto:400,700%7CRoboto+Slab%7CRoboto+Condensed' media='all' />
<link rel='stylesheet' id='flat-theme-css' href='../wp-content/themes/flat/assets/css/flat.minba3a.css?ver=1.7.2' media='all' />
<link rel='stylesheet' id='flat-style-css' href='../wp-content/themes/flat/style3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='sccss_style-css' href='../index89d4.html?sccss=1&amp;ver=4.2.2' media='all' />
<script src='../wp-includes/js/jquery/jquery4a80.js?ver=1.11.2'></script>
<script src='../wp-includes/js/jquery/jquery-migrate.min1576.js?ver=1.2.1'></script>
<script src='../wp-content/themes/flat/assets/js/flat.minba3a.js?ver=1.7.2'></script>
<!--[if lt IE 9]>
<script src='http://ieeevr.org/2016/wp-content/themes/flat/assets/js/html5shiv.min.js?ver=3.7.2'></script>
<![endif]-->
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../xmlrpc0db0.html?rsd" />
<meta name="generator" content="WordPress 4.2.2" />
<link rel='canonical' href='index.html' />
<link rel='shortlink' href='../index7cb7.html?p=840' />
<meta name="viewport" content="width=device-width, initial-scale=1"><link type="image/x-icon" href="../wp-content/uploads/2015/07/Favicon.png" rel="shortcut icon"><style type="text/css">#page:before, .sidebar-offcanvas, #secondary { background-color: #f66733; }@media (max-width: 1199px) { #page &gt; .container { background-color: #f66733; } }body { background-size: contain; }</style><style type="text/css">#masthead .site-title {font-family:Amatic SC}body {font-family:Roboto }h1,h2,h3,h4,h5,h6 {font-family:Roboto Slab}#masthead .site-description, .hentry .entry-meta {font-family:Roboto Condensed}</style><style type="text/css" id="custom-background-css">
body.custom-background { background-image: url('../wp-content/uploads/2015/07/VRBackgroundAlt.png'); background-repeat: repeat-y; background-position: top center; background-attachment: fixed; }
</style>
	</head>

<body class="page page-id-840 page-template-default custom-background" itemscope itemtype="http://schema.org/WebPage">
<div id="page">
<div class="topbanner">
<img class="topbannerimg" src="../wp-content/uploads/2016/01/Banner-w-Logos.png" alt="VR Banner with Peaches" width="100%">
</div>
	<div class="container">
		<div class="row row-offcanvas row-offcanvas-left">
			<div id="secondary" class="col-lg-3">
								<header id="masthead" class="site-header" role="banner">
										<div class="hgroup">
						<h1 class="site-title display-logo"><a href="../index.html" title="IEEE VR 2016" rel="home"><img itemprop="primaryImageofPage" alt="IEEE VR 2016" src="../wp-content/uploads/2015/07/IEEE2016-round-whitetext-01.png" /></a></h1><h2 itemprop="description" class="site-description">March 19 – 23, 2016</h2>					</div>
					<button type="button" class="btn btn-link hidden-lg toggle-sidebar" data-toggle="offcanvas" aria-label="Sidebar"><i class="fa fa-gear"> Sponsors</i></button>
					<button type="button" class="btn btn-link hidden-lg toggle-navigation" aria-label="Navigation Menu">Menu  <i class="fa fa-bars"></i></button>
					<nav id="site-navigation" class="navigation main-navigation" role="navigation">
						<ul id="menu-main-menu" class="nav-menu"><li id="menu-item-167" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-167"><a href="../index.html">Home</a></li>
<li id="menu-item-482" class="menu-item menu-item-type-custom menu-item-object-custom current-menu-ancestor current-menu-parent menu-item-has-children menu-item-482"><a href="../program/program-overview/index.html">Program</a>
<ul class="sub-menu">
	<li id="menu-item-219" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-219"><a href="../program/program-overview/index.html">Program Overview</a></li>
	<li id="menu-item-214" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-214"><a href="../program/keynote/index.html">Keynote</a></li>
	<li id="menu-item-672" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-672"><a href="../program/capstone-presentation/index.html">Capstone Presentation</a></li>
	<li id="menu-item-217" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-217"><a href="../program/papers/index.html">Papers</a></li>
	<li id="menu-item-871" class="menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-840 current_page_item menu-item-871"><a href="index.html">Posters</a></li>
	<li id="menu-item-216" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-216"><a href="../program/panels/index.html">Panels</a></li>
	<li id="menu-item-223" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-223"><a href="../program/workshop-papers/index.html">Workshops</a>
	<ul class="sub-menu">
		<li id="menu-item-427" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-427"><a href="../program/workshop-papers/third-ieee-vr-international-workshop-on-collaborative-virtual-environments-3dcve/index.html">Collaborative Virtual Environments (3DCVE)</a></li>
		<li id="menu-item-419" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-419"><a href="../program/workshop-papers/second-workshop-on-everyday-virtual-reality/index.html">Everyday Virtual Reality</a></li>
		<li id="menu-item-395" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-395"><a href="../program/workshop-papers/ieee-vr-2016-workshop-on-perceptual-and-cognitive-issues-in-ar-percar/index.html">Perceptual and Cognitive Issues in AR</a></li>
		<li id="menu-item-415" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-415"><a href="../program/workshop-papers/immersive-analytics-ia/index.html">Immersive Analytics (IA)</a></li>
		<li id="menu-item-416" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-416"><a href="../program/workshop-papers/mixed-reality-art-mra-workshop/index.html">Mixed Reality Art (MRA)</a></li>
		<li id="menu-item-468" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-468"><a href="../program/workshop-papers/ieee-virtual-reality-2016-workshop-on-k-12-embodied-learning-through-virtual-augmented-reality-kelvar/index.html">K-12 Embodied Learning through Virtual &#038; Augmented Reality (KELVAR)</a></li>
		<li id="menu-item-417" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-417"><a href="../program/workshop-papers/9th-workshop-on-software-engineering-and-architectures-for-realtime-interactive-systems/index.html">Software Engineering and Architectures for Realtime Interactive Systems</a></li>
		<li id="menu-item-418" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-418"><a href="../program/workshop-papers/ieee-vr-workshop-on-virtual-humans-and-crowds-for-immersive-environments/index.html">Virtual Humans and Crowds for Immersive Environments</a></li>
	</ul>
</li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="../program/tutorials/index.html">Tutorials</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="../program/research-demos/index.html">Research Demos</a></li>
	<li id="menu-item-222" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-222"><a href="../program/videos/index.html">Videos</a></li>
	<li id="menu-item-212" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-212"><a href="../program/exhibitors/index.html">Exhibitors</a></li>
</ul>
</li>
<li id="menu-item-169" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-169"><a href="../attend/registration/index.html">Attend</a>
<ul class="sub-menu">
	<li id="menu-item-537" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-537"><a href="../attend/registration/index.html">Registration</a></li>
	<li id="menu-item-239" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-239"><a href="../attend/venue/index.html">Venue</a></li>
	<li id="menu-item-236" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-236"><a href="../attend/accommodations/index.html">Accommodations</a></li>
	<li id="menu-item-238" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-238"><a href="../attend/transportion/index.html">Transportation</a></li>
	<li id="menu-item-1038" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1038"><a href="../attend/restaurants-2/index.html">Restaurants</a></li>
	<li id="menu-item-1039" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1039"><a href="../attend/attractions-2/index.html">Attractions</a></li>
</ul>
</li>
<li id="menu-item-168" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-168"><a href="../contribute/papers/index.html">Contribute</a>
<ul class="sub-menu">
	<li id="menu-item-362" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-362"><a href="../contribute/call-for-doctoral-consortium/index.html">Doctoral Consortium</a></li>
	<li id="menu-item-225" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-225"><a href="../contribute/exhibitors-and-supporters/index.html">Exhibitors and Supporters</a></li>
	<li id="menu-item-338" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-338"><a href="../contribute/call-for-panels/index.html">Panels</a></li>
	<li id="menu-item-229" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-229"><a href="../contribute/papers/index.html">Papers</a></li>
	<li id="menu-item-321" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-321"><a href="../contribute/call-for-posters/index.html">Posters</a></li>
	<li id="menu-item-381" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-381"><a href="../contribute/call-for-research-demos/index.html">Research Demos</a></li>
	<li id="menu-item-388" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-388"><a href="../contribute/call-for-student-volunteers/index.html">Student Volunteers</a></li>
	<li id="menu-item-354" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-354"><a href="../contribute/call-for-tutorials/index.html">Tutorials</a></li>
	<li id="menu-item-399" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-399"><a href="../contribute/call-for-videos/index.html">Videos</a></li>
	<li id="menu-item-528" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-528"><a href="../contribute/call-for-workshops/workshops/index.html">Workshops</a></li>
</ul>
</li>
<li id="menu-item-1052" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1052"><a href="../participate/presenter-instructions/index.html">Presenter Instructions</a></li>
<li id="menu-item-170" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-170"><a href="../committees/conference-committee/index.html">Committees</a>
<ul class="sub-menu">
	<li id="menu-item-240" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-240"><a href="../committees/conference-committee/index.html">Conference Committee</a></li>
	<li id="menu-item-241" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-241"><a href="../committees/program-committee/index.html">Program Committee</a></li>
	<li id="menu-item-242" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-242"><a href="../committees/steering-committee/index.html">Steering Committee</a></li>
</ul>
</li>
<li id="menu-item-1090" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1090"><a href="../awards-2/index.html">Awards</a></li>
<li id="menu-item-243" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-243"><a href="../past-conferences/index.html">Past Conferences</a></li>
<li id="menu-item-171" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-171"><a target="_blank" href="http://3dui.org/">IEEE 3DUI 2016</a></li>
<li id="menu-item-1037" class="menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-840 current_page_item menu-item-1037"><a href="index.html">Posters</a></li>
</ul>					</nav>
									</header>
				
				<div class="sidebar-offcanvas">
					<div id="main-sidebar" class="widget-area" role="complementary">
											<aside id="sfsi-widget-3" class="widget sfsi">
            <div class="sfsi_widget" data-position="widget">   
				<div id='sfsi_wDiv'></div>
                    <div class="norm_row sfsi_wDiv"  style="width:225px;text-align:left;position:absolute;"><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale'   href='javascript:void(0)' id='sfsiid_facebook' alt='Facebook' style='opacity:1' ><img alt='Facebook' title='Facebook' src='../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_facebook.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a><div class="sfsi_tool_tip_2 fb_tool_bdr sfsiTlleft" style="width:62px ;opacity:0;z-index:-1;margin-left:-47.5px;" id="sfsiid_facebook"><span class="bot_arow bot_fb_arow"></span><div class="sfsi_inside"><div  class='icon1'><a href='https://www.facebook.com/ieeevr' target='_blank'><img alt='Facebook' title='Facebook' src='../wp-content/plugins/ultimate-social-media-icons/images/visit_icons/facebook.png'  /></a></div><div  class='icon2'><div class="fb-like" data-href="http://ieeevr.org/2016/posters/" data-layout="button" data-action="like" data-show-faces="false" data-share="true"></div></div></div></div></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.twitter.com/IEEEVR' id='sfsiid_twitter' alt='Twitter' style='opacity:1' ><img alt='Twitter' title='Twitter' src='../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_twitter.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.youtube.com/user/ieeevrconf' id='sfsiid_youtube' alt='YouTube' style='opacity:1' ><img alt='YouTube' title='YouTube' src='../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_youtube.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.specificfeeds.com/widget/emailsubscribe/MTU5MDQx/OA==/' id='sfsiid_email' alt='Email general chairs' style='opacity:1' ><img alt='Email general chairs' title='Email general chairs' src='../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_email.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div></div ><div id="sfsi_holder" class="sfsi_holders" style="position: relative; float: left;width:100%;z-index:-1;"></div ><script>jQuery(".sfsi_widget").each(function( index ) {
					if(jQuery(this).attr("data-position") == "widget")
					{
						var wdgt_hght = jQuery(this).children(".norm_row.sfsi_wDiv").height();
						var title_hght = jQuery(this).parent(".widget.sfsi").children(".widget-title").height();
						var totl_hght = parseInt( title_hght ) + parseInt( wdgt_hght );
						jQuery(this).parent(".widget.sfsi").css("min-height", totl_hght+"px");
					}
				});</script>	      		<div style="clear: both;"></div>
            </div>
            					</aside>
					<aside id="text-6" class="widget widget_text">
						<h3 class='widget-title'>Exhibitors and Supporters</h3>
			<div class="textwidget"><h6>Diamond Level</h6>
<center>
<img src="../wp-content/uploads/2015/10/NSF.png" width="80%" align="middle"  style="background-color: white;">

<img src="../wp-content/uploads/2016/02/ClemsonCES.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/SOCNewLogo.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Silver Level</h6>
<center>
<img src="../wp-content/uploads/2016/02/ART-logo-black_300dpi_A4.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/coeLogo.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Bronze Level</h6>
<center>
<img src="../wp-content/uploads/2016/02/DP001.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/esi_full-colour_tagline_rgb.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/logo_big.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/MiddleVR-ImprooveReality.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/Noraxon-Logo-Transparent-Bkgrd.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/Polhemus-Green_black-tag.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/Vicon-Logo.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/02/Black-circle_Grey-orld_i_Black-viz_tagline.png" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Publisher</h6>
<center>
<img src="../wp-content/uploads/2016/02/Presence-e-logo-150x150.gif" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../wp-content/uploads/2016/03/MC_logo_color_square.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center></div>
							</aside>
											</div>
				</div>
			</div>

						<div id="primary" class="content-area col-lg-9" itemprop="mainContentOfPage">
										<div itemscope itemtype="http://schema.org/Article" id="content" class="site-content" role="main">
				
							<article id="post-840" class="post-840 page type-page status-publish hentry">
					<header class="entry-header">
						<h1 class="entry-title" itemprop="name">Posters</h1>
					</header>
										<div class="entry-content" itemprop="articleBody">
												<div class="table-responsive" id="tutorialOverview">
    <table>
        <tr>
            <td class="day"><a href="#monday"><h4>Monday<br />March 21</h4></a></td>
            <td class="day"><a href="#tuesday"><h4>Tuesday<br />March 22</h4></a></td>
        </tr>
        <tr>
            <td class="orange">
                <a href="#groupA">Group A<br />Monday, 8:00 am to 3:30 pm</a>
            </td>
            <td class="purple">
                <a href="#groupB">Group B<br />Tuesday, 8:00 am to 3:30 pm</a>
            </td>
        </tr>
    </table>
</div>

<h4 style="text-align: center;"><strong id="groupA">Group A</strong></h4>
</p><br />
<p><h4>ID: A1<br />
 A Full-Body Motion Calibration and Retargeting for Intuitive Object Manipulation in Immersive Virtual Environments</h4>
<div class="authors">
Brandon Wilson, 	University of Houston-Victoria <br />
Matthew Bounds,	University of Houston-Victoria	<br />
Alireza Tavakkoli,	Universtiy of Houston-Victoria	<br />
<strong>Presenter</strong> Brandon Wilson
</div>
<strong>Abstract: </strong> In this paper a system is proposed to combine small finger movements with the large scale body movements captured from a motion capture system. The strength of the proposed work over previous research is in the real-time and natural interactions that the virtual hands have with their environment. By being able to conform to physics, the virtual hands feel like virtual extensions of one’s own hands. This provides a higher degree of immersion and interactivity when compared to more traditional virtual reality systems.
</p>
<br />
<p><h4>ID: A2<br />
 See what I see: concepts to improve the social acceptance of HMDs
</h4>
<div class="authors">
Daniel Pohl,				Intel Corporation <br />	
Fernandez de Tejada Quemada,	Carlos	Intel Corporation <br />

<strong>Presenter</strong> Daniel Pohl
</div>
<strong>Abstract: </strong> Mobile virtual reality solutions are nowadays widely available and affordable for many smartphones, by adding a case with attached lenses around the phone to create a head-mounted display. While using these in public places or at social gatherings where the head-mounted display is given around to others, it can lead to problems regarding social acceptance, as the surrounding people are not aware of what the virtual reality user is seeing and doing. We address this problem by adding a second, front-facing screen to the head-mounted display. We build and evaluate two prototypes for this usage.
</p><br />
<p><h4>ID: A3<br />
 A Simplified Inverse Kinematic Approach for Embodied VR Applications
</h4>
<div class="authors">
Daniel Roth,		Human-Computer Interaction, Institute for Computer Science, University of Würzburg <br />
Jean-Luc Lugrin,	Human-Computer Interaction, Institute for Computer Science, University of Würzburg<br />
Julia Büser,		Institute of Media and Imaging Technology, TH Köln<br />
Gary Bente,		Communications, Arts, and Sciences, Michigan State University<br />
Arnulph Fuhrmann,	Institute of Media and Imaging Technology, TH Köln, Cologne<br />
Marc Erich Latoschik,	Human-Computer Interaction, Institute for Computer Science, University of Würzburg<br />

<strong>Presenter</strong> Daniel Roth
</div><strong>Abstract: </strong> In this paper, we compare a full body marker set with a reduced rigid body marker set supported by inverse kinematics. We measured system latency, illusion of virtual body ownership, and task load in an applied scenario for inducing acrophobia. While not showing a significant change in body ownership or task performance, results do show that latency and task load are reduced when using the rigid body inverse kinematics solution. The approach therefore has the potential to improve virtual reality experiences.
</p><br />
<p><h4>ID: A4<br />
Space-sharing AR Interaction on Multiple Mobile Devices with a Depth Camera
</h4>
<div class="authors">
Yuki Kaneto,		Saitama University	<br />
Takashi Komuro,	Saitama University<br />

<strong>Presenter</strong> Takashi Komuro
</div><strong>Abstract: </strong> We propose a markerless augmented reality system that works on multiple mobile devices. The relative positions and orientations of the devices and their individual motions are estimated from 3D information obtained by depth cameras attached to the devices. To estimate the relative positions and orientations of the devices, the system generates 2D images by looking down from above at the 3D scene obtained by the depth cameras, performs 2D registration using template matching, and obtains a transformation matrix. Using the proposed system, we created an application that enables multiple users to interact with the same virtual object.
</p><br /><p><h4>ID: A5<br />
Animated self-avatars for motor rehabilitation applications that are biomechanically accurate, low-latency and easy to use
</h4>
<div class="authors">
Mikael Dallaire-Côté,		École de technologie supérieure	<br />
	Philippe Charbonneau,	École de technologie supérieure	<br />
	Sara St-Pierre Côté,		École de technologie supérieure	<br />
	Rachid Aissaoui,		École de technologie supérieure	<br />
	David R. Labbe,		École de technologie supérieure<br />

<strong>Presenter</strong> Mikael Dallaire-Côté
</div><strong>Abstract: </strong> The emerging use of self-avatars for physical and motor rehabilitation leads to specific requirements for their real-time animation that combine properties from the fields of computer graphics and of biomechanics. We present and validate a method for animating a self-avatar in real-time that allows for high-fidelity representation of whole-body kinematics using anatomical and reproducible bone-segment definition. The method requires little setup time and has low motion-to-photon latency.
</p><br />
<p><h4>ID: A6<br />
 Monochrome Glove: A Robust Real-Time Hand Gesture Recognition Method by using a Fabric Glove with Design of Structured Markers
</h4>
<div class="authors">
Hidetoshi Ishiyama,		Cygames, Inc.	<br />
	Shuichi Kurabayashi,		Cygames, Inc.	<br />

<strong>Presenter</strong> Hidetoshi Ishiyama
</div><strong>Abstract: </strong> This paper presents a method for recognizing human-hand postures in real time, even if environment light cannot be configured appropriately. The key technology is a monochrome glove that is patterned with augmented reality markers on its palm and is also designed with a structured marker on each finger. As the glove only uses white color for the design of the patterns, it can achieve robust gesture recognition in a natural lighting environment by using a single camera to track a hand wearing the glove. The extensive experiments we conducted demonstrate the accuracy, efficiency, and robustness of our gesture recognition method.
</p><br />
<p><h4>ID: A7<br />
 Using Chromo-coded Light Fields for Augmented Reality
</h4>
<div class="authors">
Ian Schillebeeckx, 	Washington University in St. Louis	<br />
	Robert Pless,		Washington University in St. Louis	<br />

<strong>Presenter</strong> Ian Schillebeeckx
</div><strong>Abstract: </strong> This poster considers VR opportunities possible with chromo-coded light fields, created by materials like lenticular arrays whose appearance varies by viewing angle. Chromo-coded light fields use color to create geometric cues, making it cheaper, faster and more accurate to measure object pose. For high-end applications like image guided surgery, the color-cues make it possible to accurately measure pose of small objects like a scalpel. Because lenticular arrays are cheap and the color cues simplify the computation, they support new possibilities for augmented reality using smart-phones and arbitrary objects.
</p><br />
<p><h4>ID: A8<br />
 Evaluation of Hands-Free HMD-Based Navigation Techniques for Immersive Data Analysis
</h4>
<div class="authors">
Daniel	Zielasko,	Visual Computing Institute, RWTH Aachen University <br />
	Sven Horn,		Visual Computing Institute, RWTH Aachen University<br />
	Sebastian Freitag,	Visual Computing Institute, RWTH Aachen University<br /> 
	Benjamin Weyers,	Visual Computing Institute, RWTH Aachen University<br />
	Torsten W. Kuhlen, 	Visual Computing Institute, RWTH Aachen University <br />


<strong>Presenter</strong> Daniel Zielasko
</div><strong>Abstract: </strong> To use the full potential of immersive data analysis when wearing a head-mounted display, the user has to be able to navigate through the spatial data. We collected, developed and evaluated 5 different hands-free navigation methods that are usable while seated in the analyst’s usual workplace. All methods meet the requirements of being easy to learn and inexpensive to integrate into existing workplaces. We conducted a user study with 23 participants which showed that a body leaning metaphor and an accelerometer pedal metaphor performed best within the given task.
</p><br /><p><h4>ID: A9<br />
 Automatic Generation of World in Miniatures for Realistic Architectural Immersive Virtual Environments
</h4>
<div class="authors">
Andrea Bönsch,	Visual Computing Institute, RWTH Aachen University	<br />
	Sebastian Freitag,	Visual Computing Institute, RWTH Aachen University	<br />
	Torsten W. Kuhlen,	Visual Computing Institute, RWTH Aachen University<br />

<strong>Presenter</strong> Andrea Bönsch
</div><strong>Abstract: </strong>Orientation and wayfinding in architectural Immersive Virtual Environments (IVEs) are non-trivial tasks. World in Miniatures (WIMs) are an established approach to gain survey knowledge about the scene and information about the user’s relation to it. However, for large-scale scenes, scaling and occlusion issues diminish returns. Furthermore, the lack of standardized information regarding scene decompositions hampers presenting self-contained scene extracts. Therefore, we present an automatic WIM generation workflow for arbitrary in- and outdoor IVEs to provide users with meaningfully selected and scaled extracts of the IVE and corresponding context information. Additionally, a 3D user interface is provided to manipulate our WIM.
</p><br />
<p><h4>ID: A10<br />
 Combining Eye Tracking with Optimizations for Lens Astigmatism in modern wide-angle HMDs
</h4>
<div class="authors">
Daniel Pohl,		Intel Corporation	<br />
	Xucong Zhang,	Max Planck Institute for Informatics	<br />
	Andreas Bulling,	Max Planck Institute for Informatics <br />

<strong>Presenter</strong> Daniel Pohl
</div><strong>Abstract: </strong>Virtual Reality hit the consumer market with affordable HMDs. However, it quickly becomes apparent that the resolution of the built-in display panels still needs to be highly increased. To overcome the resulting higher performance demands, eye tracking can be used for foveated rendering. However, as there are lens distortions in HMDs, there are more possibilities to increase the performance with smarter rendering approaches. We present a new system using optimizations for rendering considering lens astigmatism and combining this with foveated rendering through eye tracking. Depending on the current eye gaze, this delivers a rendering speed-up of up to 20%.
</p><br />
<p><h4>ID: A11<br />
 Fast and Accurate Relocalization for Keyframe-Based SLAM Using Geometric Model Selection
</h4>
<div class="authors">
Atsunori Moteki,		Fujitsu Laboratories Ltd.	<br />
	Nobuyasu Yamaguchi,	Fujitsu Laboratories Ltd.	<br />
	Ayu Karasudani,		Fujitsu Laboratories Ltd.	<br />
	Toshiyuki Yoshitake,		Fujitsu Laboratories Ltd. <br />	


<strong>Presenter</strong> Atsunori Moteki
</div><strong>Abstract: </strong>We propose a relocalization method for keyframe-based SLAM that enables real-time and accurate recovery from tracking failures. To realize an AR-based application in a real world situation, not only accurate camera tracking but also fast and accurate relocalization from tracking failure is required. The proposed relocalization method selects two algorithms adaptively depending on the relative camera pose between a current frame and a target keyframe. In addition, it estimates a degree of false matches to speed up RANSAC-based model estimation. We present effectiveness of our method by an evaluation using public tracking dataset.
</p><br />
<p><h4>ID: A12<br />
 Psychophysical Influence on Temperature Perception by Mixed-Reality Visual Stimulation
</h4>
<div class="authors">
Satoshi Hashiguchi,	Ritsumeikan University	<br />
	Fumihisa Shibata, 	Ritsumeikan University	<br />
	Asako Kimura,	Ritsumeikan University <br />

<strong>Presenter</strong> Satoshi Hashiguchi
</div><strong>Abstract: </strong>In Mixed-Realty (MR) space, the visual appearance of a real object can be changed by superimposing a virtual object on it. We defined the changes in the visual information of a real object in MR space as “MR visual stimulation” and examine the influence of the haptic sense using MR visual stimulation. For our research, we verified the influence PresenterMR visual stimulation has on the perceived position of the temperature perception. In the experiment, we presented MR visual stimulation and temperature stimulation in different positions. Our results demonstrate that temperature perception is strongly affected by visual stimulation.
</p><br />
<p><h4>ID: A13<br />
 VR Device Time &#8211; Hi-precision Time Management By Synchronizing Times Between Devices and Host PC Through USB &#8211;
</h4>
<div class="authors">
Ryugo Kijima,		Gifu University	<br />
	Katsuya Yamaguchi,	Gifu University <br />

<strong>Presenter</strong> Katsuya Yamaguchi
</div><strong>Abstract: </strong>In the virtual reality system, accurate time management is necessary to maintain the correct relation between the displayed image and the users’ motion by the latency compensation. The timestamp concept is effective in managing the timing of the display and sensing device. For time stamping, a unified time axis is necessary among the devices and the PC. In this paper, a time synchronization system via USB was developed. This prototype was proved to have accuracy below 20 µs among multiple sensors.
</p><br />
<p><h4>ID: A14<br />
 bioSync: Wearable Haptic I/O Device for Synchronous Kinesthetic Interaction
</h4>
<div class="authors">
Jun Nishida,	University of Tsukuba	<br />
	Kenji Suzuki,	University of Tsukuba	<br />

<strong>Presenter</strong> Jun Nishida
</div><strong>Abstract: </strong>This paper presents a synchronous kinesthetic interaction through haptic input/output based on biosignal measurement and stimulation. Users are able to bi-directionally transmit kinesthetic experiences such as rigidity of joints or exertion of muscles. Such interaction would be very important in the fields of rehabilitation and sports training. In this study, we introduce a set of wearable devices that is capable of both electromyogram (EMG) measurement and electrical muscle stimulation (EMS) simultaneously on the same muscle by using common electrodes. We propose a new method for discharging the residual potential in order to enable fastest simultaneous operation (40Hz).
</p><br />
<p><h4>ID: A15<br />
 Effect of Head Mounted Display Latency on Human Stability During Quiescent Standing on One Foot
</h4>
<div class="authors">
Soma Kawamura,	Gifu University <br />
	Ryugo	Kijima,		Gifu University <br />

<strong>Presenter</strong> Soma Kawamura

</div><strong>Abstract: </strong>The purpose of this study is to reveal the effects of small latencies on head mounted display users. The subjects in this study were asked to simply and stably stand on a force plate with one foot. The speed of body sway was measured with several lags, from 1 ms to 66 ms, using an Oculus Rift DK2. The results showed that the sway speed increased monotonically with increase in the latency values. The sense of balance is regarded as a relatively direct index of quality of a VR system, including the effects of lag.
</p><br />
<p><h4>ID: A16<br />
 Depth Perception in Mirrors: The Effects of Video Based Augmented Reality in Driver&#8217;s Side View Mirrors
</h4>
<div class="authors">
Valerie Kane,		Virginia Polytechnic and State University	<br />
	Missie Smith,		Virginia Polytechnic and State University	<br />
	Gary Burnett,		University of Nottingham	<br />
	Joseph Gabbard,	Virginia Polytechnic and State University <br />	
	David Large,		University of Nottingham	<br />

<strong>Presenter</strong> Valerie Kane

</div><strong>Abstract: </strong>This study explored the effects that augmented reality graphics have on a drivers’ distance estimation/depth perception in an augmented side view mirror. The study was conducted both inside a simulator and outside in a parking lot. Sixteen participants partook in the study, 8 in the simulator and 8 in the test vehicle outside. Distance judgments were compared across four side view mirror conditions both simulator and outdoor scenarios. Results show some opportunities for AR to improve depth judgments, but further analysis is necessary.
</p><br />
<p><h4>ID: A17<br />
 Extra-normal interactions in mediated virtual environments: an investigation of an audio-visual crossed-sense modality
</h4>
<div class="authors">
Benjamin Outram,		Keio University	<br />
	Masashi Nakatani,		Keio University	<br />
	Kouta Minamizawa,		Keio University<br />


<strong>Presenter</strong> Benjamin I. Outram

</div><strong>Abstract: </strong>Unusual crossed-sense couplings can be exploited to engineer non-realistic but beneficial VEs providing high degrees of interaction and presence, and new avenues for enhancing the experience of single- and multi-user VEs. We report user&#8217;s reactions to VEs in which the sound of their voice is represented via a frequency-to-color-mapped visualization. Users reported a sense of interaction and enjoyment, but a preliminary user study has yet to show a clear link with presence. We discuss the next stages of the research which will investigate how mixed-sense modalities contribute to co-presence and collaboration in multi-user environments.

</p><br />
<p><h4>ID: A18<br />
 Evaluation of Hand and Stylus Based Calibration for Optical See-Through Head-Mounted Displays Using Leap Motion
</h4>
<div class="authors">
	Kenneth Moser,		Mississippi State University	<br />
	Edward Swan,		Mississippi State University <br />


<strong>Presenter</strong> Kenneth Moser

</div><strong>Abstract: </strong>Next generation OST HMDs promise to inclusion a variety of integrated and on-board sensors. In particular, hand tracking cameras, such as the Leap Motion, show potential for facilitating intuitive calibration procedures accessible to researchers, developers, and novice users alike. We evaluate hand and stylus based OST calibration utilizing tracking data from a Leap Motion. Our findings show performance of both methods is comparable to results from prior studies using standard environment-centric methods. Also, while our hand based calibration improved by using more contextual reticle designs, calibrations performed with a stylus yielded the most accurate and precise results over all.

</p><br />
<p><h4>ID: A19<br />
 Reducing Application-Stage Latencies of Interprocess Communication Techniques for Real-Time Interactive Systems
</h4>
<div class="authors">
Jan-Philipp Stauffert,		University of Würzburg	<br />
	Florian Niebling,		University of Würzburg	<br />
	Marc Erich Latoschik,		University of Würzburg <br />

<strong>Presenter</strong> Marc Erich Latoschik

</div><strong>Abstract: </strong>This paper analyzes latency jitter caused by typical interprocess communication (IPC) techniques commonly found in today&#8217;s systems used for VR. We use four different implementations on a Linux kernel as well as on a real-time (RT) Linux kernel to further assess if a RT variant of a multiuser multiprocess operating system can prevent latency spikes and how this behavior would apply to different programming languages and IPC techniques. We found that Linux RT can limit the latency jitter at the cost of throughput for certain implementations. Further, coarse grained concurrency should be employed to avoid adding up of scheduler latencies.

</p><br />
<p><h4>ID: A20<br />
 Analysis in Support of Realistic Timing in Animated Fingerspelling
</h4>
<div class="authors">
Nkenge Wheatland,	University of California, Riverside	<br />
	Ahsan Abdullah,	University of California, Davis	<br />
	Michael Neff,		University of California, Davis	<br />
	Sophie Jörg,		Clemson University	<br />
	Victor Zordan,		Clemson University	<br />



<strong>Presenter</strong> Nkenge Wheatland

</div><strong>Abstract: </strong>American Sign Language (ASL) fingerspelling is the act of spelling a word letter-by-letter when a specific sign does not exist to represent it. Synthesizing intelligible ASL, which includes fingerspelling, is important to create signing virtual characters for training and communicating in virtual environments or further applications. The rhythm and speed of fingerspelling play a large role in how well fingerspelling is understood. Using motion capture technologies, we record fingerspelling and analyze timing information about letters in the words. Our goal is to identify fingerspelling timing information and use it to create fingerspelling animations that are natural and understandable.

</p><br />
<p><h4>ID: A21<br />
 Effects of Vibrotactile Stimulation During Virtual Sandboarding
</h4>
<div class="authors">
Lind Stine,			Aalborg University	<br />
	Lui Thomsen,			Aalborg University <br />	
	Mie Egeberg,			Aalborg University	<br />
	Niels Christian Nilsson, 	Aalborg University	<br />
	Stefania Serafin,		Aalborg University	<br />
	Rolf Nordahl,			Aalborg University <br />


<strong>Presenter</strong> Niels Christian Nilsson

</div><strong>Abstract: </strong>This poster details a within-subjects study (n=17) investigating the effects of vibrotactile stimulation on illusory self-motion, presence and perceived realism during an interactive sandboarding simulation. Vibrotactile feedback was delivered using a low frequency audio transducer mounted underneath the board. The study compared three conditions: no vibration, constant vibration and dynamic vibration. The results suggest that constant vibrotactile feedback led to significantly more compelling self-motion illusions and a higher degree of perceived realism, than the condition devoid of vibrotactile feedback. No significant differences were found between the two conditions involving vibrotactile stimulation.

</p><br />
<p><h4>ID: A22<br />
 Estimation of Detection Thresholds for Audiovisual Rotation Gains
</h4>
<div class="authors">
Niels Christian Nilsson,	Aalborg University	<br />
	Evan Suma,			USC Institute for Creative Technologies <br />	
	Rolf Nordahl,			Aalborg University	<br />
	Mark Bolas,			USC Institute for Creative Technologies <br />	
	Stefania Serafin,		Aalborg University <br />


<strong>Presenter</strong> Niels Christian Nilsson

</div><strong>Abstract: </strong> Redirection techniques allow users to explore large virtual environments on foot while remaining within a limited physical space. However, research has primarily focused on redirection through manipulation of visual stimuli. We describe a within-subjects study (n=31) exploring if participants&#8217; ability to detect differences between real and virtual rotations is influenced by the addition of sound that is spatially aligned with its virtual source. The results revealed similar detection thresholds for conditions involving moving audio, static audio, and no audio. This may be viewed as an indication of visual dominance during scenarios such as the one used for the current study.

</p><br />
<p><h4>ID: A23<br />
 Low-Cost Raycast-based Coordinate System Registration for Consumer Depth Cameras
</h4>
<div class="authors">
Dennis Wiebusch,		Universität Würzburg	<br />
	Martin Fischbach,		Universität Würzburg	<br />
	Florian Niebling,		Universität Würzburg	<br />
	Marc Erich Latoschik,		Universität Würzburg <br />


<strong>Presenter</strong> Dennis Wiebusch

</div><strong>Abstract: </strong> We present four raycast-based techniques that determine the transformation between a depth camera&#8217;s coordinate system and the coordinate system defined by a rectangular surface. In addition, the surface&#8217;s dimensions are measured. In contrast to other approaches, these techniques limit additional hardware requirements to commonly available, low-cost artifacts and focus on simple non-laborious procedures. A preliminary study examining our Kinect v2-based proof of concept revealed promising first results. The utilized software is available as an open-source project.

</p><br />
<p><h4>ID: A24<br />
 A Handy System for Natural Composition of CG and Real Scene with Real-time Reflection of Lighting Changes
</h4>
<div class="authors">
Hirofumi Morioka,	NHK (Japan Broadcasting Corporation)	<br />
Hidehiko Okubo,	NHK (Japan Broadcasting Corporation)	<br />
Hideki Mitsumine,	NHK (Japan Broadcasting Corporation) <br />
<strong>Presenter</strong> Hirofumi Morioka
</div><strong>Abstract: </strong> A handy system is presented for creating in real time a natural television image composed of a real scene and a computer graphics object. The light positions and colors (RGB values) in the real scene are estimated and then applied to the object by using simple equipment. Objective and subjective experiments demonstrated the effectiveness of this system. An enhanced algorithm is also presented that improves the accuracy of light estimation.

</p><br />
<p><h4>ID: A25<br />
 Supporting Path Switching for Non-Player Characters in a Virtual Environment
</h4>
<div class="authors">
Mingze Xi,		The University of Newcastle, Australia	<br />
Shamus P. Smith,	The University of Newcastle, Australia <br />

<strong>Presenter</strong> Mingze Xi

</div><strong>Abstract: </strong> Realistic non-player characters (NPC) are an important component of virtual environments. However, generating NPCs with human-like behaviour can be difficult. A previously proposed pipeline used evacuation simulators to generate domain relevant NPC behaviour for virtual environments. However, the NPCs generated through this process had static behaviours and could not dynamically change pre-computed evacuation paths.
In this paper, we extend this pipeline with an approach that supports path switching for NPCs in four situations and demonstrates the pipeline in a virtual environment modelled on a large real building. A scalability test on the large building showed overall evacuation time in both source evacuation simulator and target virtual environment was consistent regardless of building size and complexity. Three test-cases demonstrate path switching for NPCs with increasing evacuation success rates through different situations. The reuse of static paths to enable dynamic NPC behaviour supports ongoing work to develop low cost and realistic training systems using game engine technology.

</p><br />
<p><h4>ID: A26<br />
 RayOnPlane: A Translation Technique Minimizing Gesture Size
</h4>
<div class="authors">
Philipp	Tiefenbacher,	TUM, Munich 	<br />
Clemens Techmer,	TUM, Munich 	<br />
Gerhard Rigoll,	TUM, Munich <br />


<strong>Presenter</strong> Philipp Tiefenbacher

</div><strong>Abstract: </strong> In this work, we propose a device-based manipulation technique named RayOnPlane, which maintains the ease of use also in case of increasing work space size. We compare this technique to the state-of-the-art device-based manipulation technique HOMER-S. An experiment incorporating different work space sizes indicates comparable performance in completion time, while minimizing gesture size as well as user frustration and physical strain.

</p><br />
<p><h4>ID: A27<br />
 Comparison of Mobile Touch Interfaces for Object Identification and Troubleshooting Tasks in Augmented Reality
</h4>
<div class="authors">
Philipp	Tiefenbacher,		TUM, Munich	<br />
Jan Gillich,			TUM, Munich	<br />
Paul Schott,			TUM, Munich <br />
Gerhard Rigoll,		TUM, Munich	<br />


<strong>Presenter</strong> Philipp Tiefenbacher

</div><strong>Abstract: </strong> This work adapts common HMD interfaces for the use on a hand-held device. The proposed interfaces focus on: Easy interaction on the mobile device and independence of the provided content to the user&#8217;s view.
We compare two AR interface techniques for object identification and three AR interfaces for troubleshooting. The results show that exocentric-based AR interfaces outperform egocentric ones in respect to completion time, walking distance and number of interactions.

</p><br />
<p><h4>ID: A28<br />
 Olfactory display using surface acoustic wave device and micropumps for wearable applications
</h4>
<div class="authors">
Kazuki	Hashimoto,		Tokyo Institute of Technology	<br />
Takamichi Nakamoto,	Tokyo Institute of Technology <br />

<strong>Presenter</strong> Kazuki Hashimoto

</div><strong>Abstract: </strong> Olfaction is expected to provide reality and a sense of immersion in multimedia contents. Therefore, an olfactory display, a gadget to present scents to one or more user(s), has been developed. A wearable olfactory display has advantage from the viewpoint of reducing the odorant diffusion into the atmosphere and is suitable for virtual reality applications. In this study, we developed a portable olfactory display using surface acoustic wave (SAW) device and micropumps. In the experiment using quartz crystal microbalance (QCM) gas sensor, we confirmed that the olfactory display can present the odorant with intended intensity.

</p><br />
<p><h4>ID: A29<br />
 Measurement of wind direction perception by the entire head
</h4>
<div class="authors">
Takuya Nakano,	Meijo University	<br />
Yasuyuki Yanagida,	Meijo University	<br />	

<strong>Presenter</strong> Takuya Nakano

</div><strong>Abstract: </strong> In many cases, when we present video to users, we present sound too. Hence, Eyesight and hearing are stimulated and we can improve the users’ sensation of presence. In addition, recently several VR systems using wind have been built to enhance presence. If we use wind, users don’t need to use several devices because we can present non-contact sensation. Therefore, the systems don’t disturb improvement of presence.
Some studies conclude that Presenterwind and video at the same moment improves presence. However, in these studies, wind sources are arranged rather sparsely. In this sparse arrangement, it is doubtful whether precise environment can be reproduced. Therefore, we examined the properties of wind direction perception at the entire head.

</p><br />
<p><h4>ID: A30<br />
 A multi-modal interactive tablet with tactile feedback, rear and lateral operation for maximum front screen visibility
</h4>
<div class="authors">
Itsuo Kumazawa,	Tokyo Institute of Technology <br />
Shu Yano,		Tokyo Institute of Technology <br />	
Souma	 Suzuki,	Tokyo Institute of Technology <br />	
Shunsuke Ono,	Tokyo Institute of Technology <br />

<strong>Presenter</strong> Itsuo Kumazawa

</div><strong>Abstract: </strong> When we use a tablet style handheld device such as a smart phone as a part of a virtual really system, its most outstanding future: the touch screen dominating the most area of the front face should be incorporated into the system effectively and beneficially. For example, the visual information displayed on the screen can be merged with the surrounding or background scenes and the intuitive touch operation can be performed in a suitable scenario. However, if the means of the interaction is limited to the touch operation, finger operation on the front screen must be performed even for unsuitable scenarios, and the fingers or hands occluding the visual information disturb our immersive experience. To deal with this situation, we propose a multi-modal interactive tablet that uses its cameras, accelerometer, track ball and pressure sensors implemented on its rear and side for operations ensuring visibility. The pressing and ball-rotating operation on the rear and the side and the tactile feedback generated by voice-coil-based actuators assist and guide the multi-modal interaction. The effectiveness of the multimodality with the rear and the side operation and the tactile feedback is evaluated by an experiment.

</p><br />
<p><h4>ID: A31<br />
 Head Mounted Projection for Enhanced Gaze in Social Interactions
</h4>
<div class="authors">
David Krum,		USC Institute for Creative Technologies	<br />
Sin-Hwa Kang,	USC Institute for Creative Technologies	<br />
Thai Phan, 		USC Institute for Creative Technologies	<br />
Lauren	Dukes, 	Google, Inc.	<br />
Mark Bolas, 		USC Institute for Creative Technologies <br />


<strong>Presenter</strong> David M. Krum

</div><strong>Abstract: </strong> Projected displays can present life-sized imagery of a virtual human character that can be seen by multiple observers. However, typical projected displays can only render that virtual human from a single viewpoint, regardless of whether head tracking is employed. This results in the virtual human being rendered from an incorrect perspective for most individuals. This could cause perceptual miscues, such as the &#8220;Mona Lisa&#8221; effect, causing the virtual human to appear as if it is simultaneously gazing and pointing at all observers regardless of their location. This may be detrimental to training scenarios in which all trainees must accurately assess where the virtual human is looking or pointing. We discuss our investigations into the presentation of eye gaze using REFLCT, a previously introduced head mounted projective display. REFLCT uses head tracked, head mounted projectors and retroreflective screens to present personalized, perspective correct imagery to multiple users without the occlusion of a traditional head mounted display. We examined how head mounted projection for enhanced presentation of eye gaze might facilitate or otherwise affect social interactions during a multi-person guessing game of &#8220;Twenty Questions.&#8221;

</p><br />
<p><h4>ID: A32<br />
 Automatic Identification of Rigidly Linked 6DoF Sensors
</h4>
<div class="authors">
Jake Fountain, 	The University of Newcastle, Australia	<br />
Shamus P. Smith,	The University of Newcastle, Australia <br />

<strong>Presenter</strong> Jake Fountain

</div><strong>Abstract: </strong> We present techniques for automatically identifying relationships between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems. The techniques allow for subsequent automatic alignment of the sensor systems, increasing the usability of modular sensor systems. Two techniques are presented and analysed in simulation and a case study for performance under varying noise and latency conditions. Good results were achieved, with each sensor identified correctly in at least 60% of estimates, or 4 times greater than random selection. After a sample collection period of around 5 seconds, the matching is performed in less than 5ms and is scalable to noisier systems by using more samples. Our methods represent a key step in creating highly accessible modular multi-device 3D systems.

</p><br />
<p><h4>ID: A33<br />
 Influence by others&#8217; opinions: social pressure from agents in immersive virtual environments
</h4>
<div class="authors">
Christos Kyrlitsias,	GET Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology	<br />
Despina Michael,	GET Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology		<br />


<strong>Presenter</strong> Christos Kyrlitsias

</div><strong>Abstract: </strong> Virtual Reality is used in fields of cognitive sciences to study participants’ reactions. In such cases, existence of other avatars in the virtual environment is a crucial factor. In this study we investigate whether agents have social influence on the participants by performing the Asch conformity experiment (1951) in an immersive virtual environment. Findings are demonstrating that participants’ response times were affected by the judgments of agents existing in the virtual environment.

</p><br /><p><h4>ID: A34<br />
 SharpView: Improved Clarity of Defocussed Content on Optical See-Through Head-Mounted Displays
</h4>
<div class="authors">
Kohei Oshima,	Nara Institute of Science and Technology	<br />
Kenneth Moser,	Mississippi State University	<br />
Damien Rompapas,	Nara Institute of Science and Technology <br />	
J. Edward Swan II,	Mississippi State University	<br />
Sei Ikeda, 		Ritsumeikan University	<br />
Goshiro Yamamoto,	Nara Institute of Science and Technology <br />	
Takafumi Taketomi,	Nara Institute of Science and Technology	<br />
Christian Sandor,	Nara Institute of Science and Technology	<br />
Hirokazu Kato, 	Nara Institute of Science and Technology <br />


<strong>Presenter</strong> Kenneth Moser

</div><strong>Abstract: </strong> Augmented reality (AR) systems, which utilize optical see-through (OST) head-mounted displays (HMDs), are becoming more common place in the consumer market, especially with the current availability of several consumer level options, and the promise of additional, more advanced, devices on the horizon. Despite their growing popularity, the current generation of OST HMDs are still prone to a number of issues which diminish the impact of their utility. Our work aims to investigate the impact of one of these issues, the visual blur caused by simultaneously viewing AR and environmental objects at differing focal distances. In this paper, we investigate the impact of focus blur on user perception of AR content and also present a novel technique, termed SharpView, for mitigating these effects. Our experimental results reveal that the SharpView method correctly models the expected level of focus blur perceived by the user and provides a more perceptually correct image over an uncorrected view. Our findings also show that even though users are sensitive to the perceptual effects of focal disparity, their performance in an AR matching task was not negatively impacted even in the presence of significant blur.

</p><br /><p><h4>ID: A35<br />
 The Effect of Multi-Sensory Cues on Performance and Experience During Walking in Immersive Virtual Environments
</h4>
<div class="authors">
Mi Feng,		Worcester Polytechnic Institute	<br />
Arindam Dey,		HIT Lab Australia, University of Tasmania	<br />
Robert	Lindeman,	HIT Lab NZ <br />

<strong>Presenter</strong> Mi Feng

</div><strong>Abstract: </strong> To examine the effects of multi-sensory cues during nonfatiguing walking in immersive virtual environments, we selected sensory cues including movement wind, directional wind, footstep vibration, and footstep sounds, and investigated their influence and interaction with each other. We developed a virtual reality system with non-fatiguing walking interaction and low-latency, multi-sensory feedback, and used it to conduct two successive experiments measuring user experience and performance through a triangle-completion task. We noticed some positive effects due to the addition of footstep vibration on task performance, and saw significant improvement in reported user experience due to the added wind and vibration cues.

</p><br />
<p><h4>ID: A36<br />
 Spatial Consistency Perception in Optical- and Video-See-Through Head-Mounted Augmentations
</h4>
<div class="authors">
Alexander Plopski, 	Osaka University	<br />
Kenneth R. Moser, 	Mississippi State University <br />	
Kiyoshi Kiyokawa,	Osaka University	<br />
J. Edward Swan II,	Mississippi State University <br />	
Haruo Takemura,	Osaka University <br />


<strong>Presenter</strong> Kenneth R. Moser

</div><strong>Abstract: </strong> Correct spatial alignment is an essential requirement for convincing augmented reality experiences. Registration error, caused by a variety of systematic, environmental, and user influences decreases the realism and utility of head mounted display AR applications. Focus is often given to rigorous calibration and prediction methods seeking to entirely remove misalignment error between virtual and real content. Unfortunately, producing perfect registration is often simply not possible. Our goal is to quantify the sensitivity of users to registration error in these systems, and identify acceptability thresholds at which users can no longer distinguish between the spatial positioning of virtual and real objects. We simulate both video see-through and optical see-through environments using a projector system and experimentally measure user perception of virtual content misalignment. Our results indicate that users are less perceptive to rotational errors overall and that translational accuracy is less important in optical see-through systems than in video see-through.

</p><br />
<p><h4>ID: A37<br />
 Through the Eyes of a Bystander: The Promise and Challenges of VR as a Bullying Prevention Tool
</h4>
<div class="authors">
Kelly McEvoy,		Ultraflex Systems	<br />
Oyewole Oyekoya,	Clemson University	<br />
Adrienne Holz Ivory,	Virginia Tech	<br />
James D. Ivory,	Virginia Tech	<br />

<strong>Presenter</strong> James D. Ivory
</div>
<strong>Abstract: </strong>Two studies explored the potential of virtual reality (VR) in bystander-focused bullying prevention campaigns. An experiment compared responses to three versions of a bullying scenario in which users (N = 78) were placed in the perspective of a bystander: customized VR, non-customized VR, and video. Measures included empathy, attitudes toward bullying victims and bullying, anticipated future behavior, presence, and other perceptions of bullying. The only significant effects observed were on feelings of empathy, with scores in the video condition higher than in the other two conditions, and on perceptions of bullying as a problem in participants’ schools, again with scores highest in the video condition. These results were further explored in a follow-up qualitative focus group study (N = 10). Findings from both studies suggest that to elicit empathy-related responses, VR simulations should use photorealistic graphics, employ interactive features, and make customization prominent and carefully tailored. Lessons learned could inform the use of virtual reality in future campaigns.

</p><br />
<p><h4>ID: A38<br />
 OST Rift: Temporally Consistent Augmented Reality with a Consumer Optical See-Through Head-Mounted Display
</h4>
<div class="authors">
Yuta Itoh,		Technische Universität München	<br />
Jason	Orlosky, 	Osaka University	<br />
Kiyoshi Kiyokawa,	Osaka University	<br />
Manuel Huber,	Technische Universität München <br />	
Gudrun Klinker,	Technische Universität München<br />
<strong>Presenter</strong> Yuta Itoh

</div><strong>Abstract: </strong> We present an off-the-shelf, low-latency Optical See-through Head-Mounted Displays (OST-HMD) for Augmented Reality (AR). Temporally consistent visualization is crucial for realizing immersive AR experiences. This is challenging since it requires both accurate head-tracking and low-latency rendering of AR content. Building a system which meets both constraints usually requires experts on computer vision/graphics and expensive display hardware. This work demonstrates that such high spatio-temporal fidelity is achievable with commodity hardware available today.
We build a custom OST-HMD system that consists of a virtual reality HMD, i.e., the Oculus Rift DK2, and half-mirror optics, and adapt the rendering pipeline in order to integrate the OST-HMD calibration framework. An evaluation with a user-perspective camera shows that the system achieves mean temporal error of

</p><br />
<p><h4>ID: A39<br />
 Acting Together: Joint Pedestrian Road Crossing in an Immersive Virtual Environment
</h4>
<div class="authors">
Yuanyuan Jiang,	The University of Iowa	<br />
Elizabeth O&#8217;Neal,	The University of Iowa	<br />
Junghum Paul	Yon,	The University of Iowa	<br />
Luke Franzen,		The University of Iowa	<br />
Pooya Rahimian,	The University of Iowa	<br />
Jodie Plumert,		The University of Iowa	<br />
Joseph Kearney,	The University of Iowa<br />


<strong>Presenter</strong> Yuanyuan Jiang

</div><strong>Abstract: </strong> We investigated how two people jointly coordinate their decisions and actions in a co-occupied, large-screen virtual environment. The task for participants was to physically cross a virtual road with continuous traffic without getting hit by a car. Participants performed this task either alone or with another person (see Fig.1). We found that pairs often crossed the same gap together and closely synchronized their movements when crossing. Pairs also chose larger gaps than individuals to accommodate the extra time needed to cross through gaps together. These results reveal how two people interact and coordinate their behaviors in performing whole-body, joint motions. This study also provides a foundation for future studies examining joint actions in shared VEs where participants are represented by graphic avatars.

</p><br />
<p><h4>ID: A40<br />
 Exploring the Perception of Co-Location Errors during Tool Interaction in Visuo-Haptic Augmented Reality
</h4>
<div class="authors">
Ulrich Eck University of South Australia, Mawson Lakes, South Australia, Australia<br />

Ulrich Eck, 		University of South Australia <br />
Liem Hoang,		Nara Institute of Science and Technology<br />	
Christian Sandor, 	Nara Institute of Science and Technology<br />	
Goshiro Yamamoto,	Nara Institute of Science and Technology	<br />
Takafumi Taketomi, 	Nara Institute of Science and Technology	<br />
Hirokazu Kato,	Nara Institute of Science and Technology	<br />
Hamid	Laga,		University of South Australia<br />


<strong>Presenter</strong> Liem Hoang

</div><strong>Abstract: </strong> Co-located haptic feedback in mixed and augmented reality environments can improve realism and user performance, but it also requires careful system design and calibration. In this poster, we determine the thresholds for perceiving co-location errors through two psychophysics experiments in a typical fine-motor manipulation task. In these experiments we simulate the two fundamental ways of implementing VHAR systems: first, attaching a real tool; second, augmenting a virtual tool. We determined the just-noticeable co-location errors for position and orientation in both experiments and found that users are significantly more sensitive to co-location errors with virtual tools. Our overall findings are useful for designing visuo-haptic augmented reality workspaces and calibration procedures.

</p><br />
<p><h4>ID: A41<br />
 The Rainbow Marker: An AR Marker with Planar Light Probe based on Structural Color Pattern Matching
</h4>
<div class="authors">
Yuki Uranishi,		Kyoto University	<br />
Masataka Imura,	Kwansei Gakuin University	<br />
Tomohiro Kuroda,	Kyoto University <br />

<strong>Presenter</strong> Yuki Uranishi

</div><strong>Abstract: </strong> This paper proposes The Rainbow Marker, a planar marker for estimating the direction of a light source using a structural color. A structural color is a color produced by microscopically structured surfaces that vary in appearance according to the viewpoint, the direction and the spectrum of the light source. The proposed marker contains a planar material which causes structural coloration. The direction of the light source is estimated by structural color pattern matching between an input pattern and referential color patterns.
In this paper, two types of the marker were implemented, with a grating sheet and with a holographic sheet, to demonstrate that the proposed method is applicable in the field of augmented reality.

</p><br />
<p><h4>ID: A42<br />
 Groupnect: Integrating Group Interaction into Large Display System
</h4>
<div class="authors">
Hao Jiang, 	Institute of Computing Technology, Chinese Academy of Sciences<br />
Chang Gao,	Institute of Computing Technology, Chinese Academy of Sciences <br />
Tianlu Mao,	Institute of Computing Technology, Chinese Academy of Sciences<br />
Hui Li,		Sichuan University	<br />
Zhaoqi Wang,	Institute of Computing Technology, Chinese Academy of Sciences<br />


<strong>Presenter</strong> Hao Jiang

</div><strong>Abstract: </strong> Large display systems have been successfully applied in virtual reality domains because they can provide full sense of immersion through large visual space and high display resolution. However, only a few users can interact with these systems by using pen-like or marker-based devices. In addition, user experience and application mode are constrained in many areas. In this paper, we propose a novel application framework called &#8221;Groupnect&#8221;, which gives users unique experience of group interaction in a large display system. By using optical tracking and 3D gesture recognition technologies, our approach can automatically recognize gesture-based control signals for 12 users simultaneously. And the backend system can trigger corresponding actions in real time. We conduct a user study and compare the results with a standard interaction mode. The results demonstrate that our approach greatly increases recorded objective activities and subjective efforts. Moreover, the physical and mental participation of users can be promoted by Groupnect. It indicates great potential to design novel applications in entertainment, education and training areas.</p>
<br />
<h4 style="text-align: center;"><strong id="groupB">Group B</strong></h4>

<br /><p><h4>ID: B1<br />
 Measurement of Head Mounted Display&#8217;s Latency in Rotation and Side Effect Caused by Lag Compensation by Simultaneous Observation &#8211; An Example Result Using Oculus Rift DK2
</h4>
<div class="authors">
Ryugo Kijima,		Gifu University	<br />
Kento Miyajima,	Gifu University<br />


<strong>Presenter</strong> Kento Miyajima

</div><strong>Abstract: </strong> Latency is an important specification of the Head Mounted Display (HMD). In this paper, the s proposed the measurement methods to evaluate the average latency as well as the effects/ side effects of the lag compensation. The Oculus Rift DK2 was selected for measurement. The results showed that the average latency of an DK2 without compensation was about 26.3 ms and that with full compensation was 1 ms. The rest part of the dynamic response was observed and evaluated by subtracting the measured latency from the observed trajectory. The side effect of Timewarp was observed as the spike like angular error.</p><br />
<p><h4>ID: B2<br />
 Speaking Haptics: Proactive Haptic Articulation for Intercommunication in Virtual Environments
</h4>
<div class="authors">
Victor Adriel Oliveira,		Universidade Federal do Rio Grande do Sul <br />	
Anderson Maciel,		Universidade Federal do Rio Grande do Sul	<br />
Luciana Nedel,		Federal University of Rio Grande do Sul (UFRGS)	<br />


<strong>Presenter</strong> Luciana Nedel

</div><strong>Abstract: </strong> Communication is crucial in collaborative tasks. Multimodal strategies are commonly applied to complement, reinforce and disambiguate information exchange. However, although multimodal communication is commonplace in Collaborative Virtual Environments, the proactive use of touch for intercommunication is surprisingly neglected regardless its importance for communication. In this paper, we look up to elements present in speech articulation to introduce the proactive haptic articulation as a novel approach for communication in CVEs. We defend the hypothesis that elements present in natural language, when added to the design of the vibrotactile vocabulary, should provide an expressive medium for intercommunication. Moreover, we hypothesize that the ability to render tactile cues to a teammate will encourage users to adapt a given vocabulary spontaneously during its use. We implemented a case study around a collaborative puzzle task to demonstrate the use of such vocabulary. Results show that the proactive haptic articulation provided a way for participants to autonomously and dynamically adapt the provided tactile vocabulary to attend their communication needs during the task.
</p><br />
<p><h4>ID: B3<br />
 Faster Feedback for Remote Scene Viewing with Pan-Tilt Stereo Camera
</h4>
<div class="authors">
Yi Ren,	University of North Carolina at Chapel Hill	<br />
Henry Fuchs,	University of North Carolina at Chapel Hill<br />
<strong>Presenter</strong> Yi Ren

</div><strong>Abstract: </strong> We demonstrate a remote scene viewing system for telepresence purposes. The system is based on a pan-tilt stereo camera that captures stereo video and transfers it to a remote user over a network. On the user end, the live stereo video is processed and displayed in a Head-Mounted Display. Faster feedback can be achieved through latency compensation. Using a wider field-of-view, higher resolution camera, the appropriate subset of the image is selected and displayed. We introduce the hardware configuration and software framework for the system and a method to calculate the homography between the camera image space and user head image space. Our perceived latency of the system is estimated to be 50-100ms.
</p><br />
<p><h4>ID: B4<br />
 Effects of field of regard and stereoscopy and the Validity of MR simulation for Visual Analysis of scientific data
</h4>
<div class="authors">
Wallace S. Lages,	Virginia Tech	<br />
Bireswar Laha,	Stanford University	<br />
Wesley Miller,		Brown University <br />	
Johannes Novotny,	Brown University <br />	
John J. Socha, 	Virginia Tech	<br />
David H. Laidlaw,	Brown University <br />	
Doug Bowman,	Virginia Tech <br />

<strong>Presenter</strong> Wallace Lages

</div><strong>Abstract: </strong> We report the findings of a study designed to evaluate the effect of stereopsis and field of regard (FOR) in two different mixed reality (MR) simulation platforms: a head-mounted display (HMD) and a CAVE. We compared the performance of participants on two levels of stereopsis (mono and stereo) and two levels of FOR (90 degrees and 270 degrees) using a variety of scientific visualization tasks. Among the findings, we observed that not all the effects were consistent between the platforms. Stereo alone or in combination with higher FOR improved completion time on both platforms. However, adding stereo solely reduced the accuracy of the participants on the CAVE and improved on the HMD. Our findings extend prior knowledge on the contribution of visual fidelity components and suggests potential limits on MR simulation between platforms.
</p><br />
<p><h4>ID: B5<br />
 Exploring Social Presence Transfer in Real-Virtual Human Interaction
</h4>
<div class="authors">
Salam Daher,		University of Central Florida<br />
Kangsoo Kim,		University of Central Florida<br />
Myungho Lee,		University of Central Florida<br />
Andrew Raij,		University of Central Florida<br />
Ryan Schubert,	University of Central Florida<br />
Jeremy Bailenson,	Stanford University<br />
Greg Welch,		University of Central Florida<br />	



<strong>Presenter</strong> Salam Daher

</div><strong>Abstract: </strong> We explore whether a peripheral observation of apparent mutual social presence between a real human (RH) and a virtual human (VH) can in turn increase a subject’s sense of social presence with the VH. In other words, we explore whether social presence can “transfer” from one RH-VH interaction to another. Specifically we carried out an experiment where human subjects were asked to play a game with a VH. Approximately half of the subjects were exposed to a brief but apparently engaging conversation between an RH and the VH as they entered the game room. For the subjects exposed to the brief RH-VH interaction, both an emotional connection and the attentional allocation dimension of social presence for the VH were found to be significantly higher compared to those who were not. We describe the motivation, the experiment, and the results.
</p><br />
<p><h4>ID: B6<br />
 Visual Feedback to Improve the Accessibility of Head Mounted Displays for Persons with Balance Impairments
</h4>
<div class="authors">
Sharif Mohammad Shahnewaz Ferdous,	University of Texas at San Antonio<br />
Imtiaz Muhammad Arafat,			University of Texas at San Antonio<br />
John Quarles, 				University of Texas at San Antonio<br />


<strong>Presenter</strong> Sharif Mohammad Shahnewaz Ferdous

</div><strong>Abstract: </strong> The objective of this research is to improve the accessibility of Head-Mounted Displays (HMDs) for users with balance impairments while they are in immersive Virtual Environments (VEs). Previous research has shown that most users experience some imbalance in a fully immersive VE. However, this imbalance is significantly worse in users with balance deficits. Thus, this research aims to determine an effective visual feedback technique to improve balance of persons while using VEs to improve the accessibility of HMDs. In order to do that, we conducted a study with seven users without impairment and seven users with balance impairments due to Multiple Sclerosis (MS). We investigated how a static reference frame (SRF) (e.g., a cross-hair always rendered in the same position on the user&#8217;s display screen) impacts the participants&#8217; balances in VR. Results indicate that a SRF significantly improves balance in VR for users with MS. Based on these results, we propose guidelines for designing more accessible VEs for persons with balance impairments.
</p><br />
<p><h4>ID: B7<br />
 Anchoring 2D Gesture Annotations in Augmented Reality
</h4>
<div class="authors">
Benjamin Nuernberger,	University of California, Santa Barbara <br />
Kuo-Chin Lien,		University of California, Santa Barbara <br />
Tobias	Hollerer,		University of California, Santa Barbara<br />
Matthew Turk, 		University of California, Santa Barbara<br />


<strong>Presenter</strong> Benjamin Nuernberger

</div><strong>Abstract: </strong> Augmented reality enhanced collaboration systems often allow users to draw 2D gesture annotations onto video feeds to help collaborators to complete physical tasks. This works well for static cameras, but for movable cameras, perspective effects cause problems when trying to render 2D annotations from a new viewpoint in 3D. In this paper, we present a new approach towards solving this problem by using gesture enhanced annotations. By first classifying which type of gesture the user drew, we show that it is possible to render annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods.
We first determined a generic vocabulary of important 2D gestures for remote collaboration by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel system to automatically handle the top two 2D gesture annotations&#8212;arrows and circles. Arrows are handled by identifying their anchor points and using surface normals for better perspective rendering. For circles, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our approach outperforms previous methods in terms of better conveying the original drawing&#8217;s meaning from different viewpoints.
</p><br />
<p><h4>ID: B8<br />
 New Hybrid Projection to Widen the Vertical Field of View with Large Screen to Improve the Perception of Personal Space in Architectural Project Review
</h4>
<div class="authors">
Sabah	Boustila,		Université de Strasbourg	<br />
Antonio Capobianco, 		Université de Strasbourg	<br />
Olivier	Génevaux,		Université de Strasbourg	<br />
Dominique Bechmann,	Université de Strasbourg <br />


<strong>Presenter</strong> Sabah Boustila

</div><strong>Abstract: </strong> In architectural project review, the perception of the near surrounding ground is important for the evaluation of the virtual environment (VE). This near surrounding ground is missing when using wall screens. To address this problem we suggest increasing the Vertical Field of View (VFoV). One solution is the use of the rendering approach such as non-planar projection. However, simply increasing the FoV of the rendering leads to much distortion in the VE which is not suitable for architectural project review. We propose a new hybrid projection combining a perspective projection in the center of the screen and a cylindrical-like projection on the top and bottom boarders. By this, we increase VFoV without incurring large deformations to preserve the perception of distances and to allow seeing the surrounding ground. We also report the results of an experiment we conducted to evaluate distance perception with our projection.
</p><br />

<p><h4>ID: B9<br />
Vestibulohaptic passive stimulation for a walking sensation	
</h4>
<div class="authors">
Yasushi Ikei, 		Tokyo Metropolitan University <br />
Shunki Kato, 		Tokyo Metropolitan University <br />
Kohei Komase, 	Tokyo Metropolitan University <br />
Shogo Imao, 		Tokyo Metropolitan University <br />
Sho Sakurai, 		Tokyo Metropolitan University, The University of Tokyo <br />
Tomohiro Amemiya, 	NTT <br />
Michiteru Kitazaki, 	Toyohashi University of Technology <br />
Koichi Hirota, 	The University of Electro-Communications <br />
				
<strong>Presenter</strong>	Sho Sakurai

</div><strong>Abstract: </strong> 	This paper describes a passive stimulation of a body to evoke a walking sensation using a vestibular and haptic device while the real body of the user is sitting. It imparts a pseudo body image to the user through the real (physical) body of the user as a part of the virtual reality (VR) display system. The created walking sensation was evaluated by nine factors to analyze the complex nature of the walking sensation.
</p><br />


<p><h4>ID: B10<br />
 Is this bridge safe? Evaluation of Audiovisual Cues for a Walk on a Small Bridge Over a Canyon.
</h4>
<div class="authors">
Erik Sikström,		Aalborg University Copenhagen	<br />
Niels Nilsson,		Aalborg University Copenhagen	<br />
Amalia De Goetzen,	Aalborg University Copenhagen	<br />
Stefania Serafin,	Aalborg University Copenhagen <br />

<strong>Presenter</strong> Erik Sikström

</div><strong>Abstract: </strong> This paper presents two within-subjects studies (n=23) exploring how different combinations of visual and auditory feedback influence perceived realism, virtual self-perception and the experience of safety during walks on a virtual platform suspended over a canyon. In the first study, the frequency factor of the footstep sounds was altered and the visual appearance was changed between a newly built wooden bridge and an old bridge with a weaker structure and broken planks. In the second study, the sounds of creaking wood were added to the footstep sounds in half of the trails and compared against footsteps without creaking sounds. Moreover, the frequency factor of the frequency controls for footsteps was also manipulated between trails, but the visual appearance of the bridge was limited to the model of the old broken bridge.
</p><br />
<p><h4>ID: B11<br />
 Avatar Realism and Social Interaction Quality in Virtual Reality
</h4>
<div class="authors">
Daniel Roth, 		Human-Computer Interaction, Institute for Computer Science, University of Würzburg 	<br />
Jean-Luc Lugrin,	Human-Computer Interaction, Institute for Computer Science, University of Würzburg<br />
Dmitri Galakhov,	Institute of Media and Imaging Technology, TH Köln	<br />
Arvid Hofmann,	Media- and Communication Psychology, Department of Psychology, University of Cologne<br />
Gary Bente,		Communications, Arts, and Sciences, Michigan State University <br />
Marc Erich Latoschik,	Human-Computer Interaction, Institute for Computer Science, University of Würzburg	<br />
Arnulph Fuhrmann,	Institute of Media and Imaging Technology, TH Köln <br />


<strong>Presenter</strong> Daniel Roth

</div><strong>Abstract: </strong> In this paper, we describe an experimental method to investigate the effects of reduced social information and behavioral channels in immersive virtual environments with full-body avatar embodiment. We compared physical-based and verbal-based social interactions in real world (RW) and virtual reality (VR). Participants were represented by abstract avatars that did not display gaze, facial expressions or social cues from appearance. Our results show significant differences in terms of presence and physical performance. However, differences in effectiveness in the verbal-based task were not present. Participants appear to efficiently compensate for missing social and behavioral cues by shifting their attentions to other behavioral channels.
</p><br />
<p><h4>ID: B12<br />
 Supporting Multiple Immersive Configurations Using a Shape-Changing Display
</h4>
<div class="authors">
Anthony Steed,	University College London	<br />

<strong>Presenter</strong> Anthony Steed

</div><strong>Abstract: </strong> Immersive displays for virtual reality systems can be roughly classified into spatially immersive displays (similar to CAVE-like dis- plays or large-screen simulators) or head-mounted displays. The former type is usually static in spatial configuration and configured to support a small group of users. The latter supports only a single user. We propose a new class of actuated, reconfigurable display that can support both small groups and individual users: in particular we suggest a robotic display that can change shape. The display can change shape to support different usage conditions, and can also move rapidly to give a larger apparent field of view for an individual user. We explore the potential advantages of a display that can move independently from its user(s), and we present a prototype that demonstrates some of the potential use scenarios.
</p><br />
<p><h4>ID: B13<br />
 Using Virtual Environments to Evaluate Assumptions of the Human Visual System
</h4>
<div class="authors">
Eric Palmer,		Purdue University	<br />
Aaron Michaux,	Purdue University	<br />
Zygmunt Pizlo,	Purdue University <br />


<strong>Presenter</strong> Eric Palmer

</div><strong>Abstract: </strong> Virtual reality applications provide an opportunity to test human vision in well-controlled scenarios that would be difficult or impossible to generate in real physical spaces. This paper presents a study intended to evaluate the importance of possible assumptions made by the human visual system. Using a CAVE simulation, participants viewed and counted virtual furniture objects in a variety of experimental manipulations. The assumption of uprightness against inversion, or the “gravity constraint,” was identified as a significant assumption of the visual system (p &lt; 0.001). Monocular vs. binocular vision was also demonstrated as an important factor in this study (p = 0.01), while color vs. grayscale did not have a significant impact on task performance (p = 0.16). By including the binocular cue, and the assumption about the direction of gravity, the scene reconstruction produced by our computer vision model is reliable. The model can detect and count symmetrical objects in a 3D real scene and then recover their 3D shapes.
</p><br />
<p><h4>ID: B14<br />
 Acoustic Redirected Walking with Auditory Cues by Means of Wave Field Synthesis
</h4>
<div class="authors">
Malte Nogalski,	University of Applied Sciences Hamburg	<br />
Wolfgang Fohl,	University of Applied Sciences Hamburg <br />


<strong>Presenter</strong> Malte Nogalski

</div><strong>Abstract: </strong> We present an experiment to identify detection thresholds for acoustic redirected walking by means of a wave field synthesis system. The most natural way to navigate an avatar through an immersive virtual environment (IVE) is by copying the tracked physical movements of a user. Redirected walking offers an approach to tackle the discrepancy between the potentially infinite IVE and the generally limited available physical space or tracking area, by applying manipulations, such as rotations or translations, to the IVE in form of gains to the user’s movements.
39 blindfolded test subjects performed 2777 constant stimulus trials with various amounts of rotation and curvature gains. The test subjects were divided into four groups with different knowledge of the experiment, and one group performed two-alternative-forced-choice tasks, while the others could give feedback freely. The detection thresholds were greatly dependent on the groups i.e., the knowledge of the experiment. The 25% detection threshold was reached by the most relevant test group at gains that up-scaled rotations by 5%, down-scaled them by 37.5%, and bend a straight path into a circle with a radius of 5.71 meters. Almost no signs of simulator sickness could be observed.
</p><br />
<p><h4>ID: B15<br />
 A methodology for reducing the time to generate virtual electric substations
</h4>
<div class="authors">
Alexandre Carvalho,		Universidade Federal de Uberlândia <br />
Leandro Mattioli,		Universidade Federal de Uberlândia<br />
Camilo Barreto,		Universidade Federal de Uberlândia<br />
Milton Miranda Neto,		Universidade Federal de Uberlândia<br />
Paulo Prado,			Companhia Energética de Minas Gerais – CEMIG Gerson Flavio <br />	Mendes de Lima,	Universidade Federal de Uberlândia <br />
Edgard Lamounier,		Universidade Federal de Uberlândia <br />
Alexandre Cardoso,		Universidade Federal de Uberlândia <br />


<strong>Presenter</strong> Alexandre Cardoso

</div><strong>Abstract: </strong> One of the great challenges in the development of Virtual Reality applications is to create the illusion of being in a different space. This is especially true when we are dealing with critical systems in engineering such as the control and monitoring of power substations. In this work, an electric power energy company is a research partner with more than 50 electric substations. Therefore, time to model all these substations, with a high-level of required photorealism, is an essential issue. To achieve this goal, a methodology is presented. First, the methodology proposes a protocol for acquiring data from field components (CADs, satellite images, manufacturer sheets etc.) to model faithful electric components by means of dimensioning and angles. Next, rules such as cable connectors positioning and monitoring of the amount of polygons (low-poly) are established. In addition, since each electric substation has circuit arrangements composed by different electric components, a pattern recognition tool has been developed to extract information from 2D basic plants in order to generate automatic positioning of components within a virtual substation. Also, considering the need for control and monitoring of the electric system, in real time, a set of interface templates are provided to support direct access to data from supervisory system (SCADA), without the loss of immersion and navigation which are imperative for Virtual Reality applications. Experiments have shown that this initiative reduces mental efforts of employees when operating the system. In the very first trials to generate a virtual electric substation a lot of work and time have been spent by our research team. After the establishment of the proposed methodology, results show that the time to generate new substations has been reduced by the order of 83%.
</p><br />
<p><h4>ID: B16<br />
 Induction of Linear and Circular Vection in Real and Virtual Worlds
</h4>
<div class="authors">
Bobby Bodenheimer,	Vanderbilt University	<br />
Yiming	Wang,		Vanderbilt University	<br />
Divine Maloney,	University of the South	<br />
John Rieser,		Vanderbilt University <br />

<strong>Presenter</strong> Bobby Bodenheimer

</div><strong>Abstract: </strong> Vection is the illusion of self-motion, usually induced by a visual stimulus. It is important in virtual reality because inducing it in motion simulations can lead to improved experiences. In this poster we examine linear and circular vection in commodity level head-mounted displays. We compare the experience of circular vection induced through a real world stimulus, an optokinetic drum, with that experienced through a virtual stimulus. With virtual stimuli, we also compare circular vection with linear horizontal and linear vertical vection. Finally, we examine circular and linear vection in more naturalistic virtual environments. Linear vection was induced more rapidly than any other type, but circular vection occurs more rapidly with a real world stimulus than a virtual one. Our results have practical application and can inform virtual reality design that uses head-mounted display technology and wishes to establish vection.
</p><br />
<p><h4>ID: B17<br />
 Detecting Movement Patterns from Inertial Data of a Mobile Head-Mounted-Display for Navigation via Walking-in-Place
</h4>
<div class="authors">
Thies Pfeiffer,		Center of Excellence Cognitive Interaction Technology	<br />
Aljoscha Schmidt,	Bielefeld University	<br />
Patrick Renner,	Center of Excellence Cognitive Interaction Technology <br />

<strong>Presenter</strong> Patrick Renner

</div><strong>Abstract: </strong> While display quality and rendering for Head-Mounted-Displays (HMDs) has increased in quality and performance, the interaction capabilities with these devices are still very limited or relying on expensive technology. Current experiences offered for mobile HMDs often stick to dome-like looking around, automatic or gaze-triggered movement, or flying techniques.
We developed an easy to use walking-in-place technique that does not require additional hardware to enable basic navigation, such as walking, running, or jumping, in virtual environments. Our approach is based on the analysis of data from the inertial unit embedded in mobile HMDs. In a first prototype realized for the Samsung Galaxy Gear VR we detect steps and jumps. A user study shows that users novice to virtual reality easily pick up the method. In comparison to a classic input device, using our walking-in-place technique study participants felt more present in the virtual environment and preferred our method for exploration of the virtual world.
</p><br />
<p><h4>ID: B18<br />
 Bringing Basic Accessibility Features to Virtual Reality Context
</h4>
<div class="authors">
Mauro Teófilo,		SIDIA, Samsung Research Institute, Manaus, AM, Brazil	<br />
Josiane Nascimento,	SIDIA, Samsung Research Institute, Manaus, AM, Brazil<br />
Jonathan Santos,	SIDIA, Samsung Research Institute, Manaus, AM, Brazil	<br />
Yves Jacques,	SIDIA, Samsung Research Institute, Manaus, AM, Brazil	<br />
André Souza,		The University of Alabama	<br />
Daniel Nogueira,	SIDIA, Samsung Research Institute, Manaus, AM, Brazil <br />


<strong>Presenter</strong> Mauro Teófilo

</div><strong>Abstract: </strong> Virtual reality is an experience, often generated by computer, that brings immersive environments that can be interacted with. Since 2014, the spread of VR technology created a content demand as well as new paradigms of interactions. One of the key aspects of this new paradigms is that they consider the use of virtual reality by visually-impaired people. In HCI, accessibility features are special computer functions that help people with disabilities to use technology more easily. This paper introduces Virtual Reality (VR) basic scenarios of accessibility tools like zooming, negative colors, auto reading, text-to-speech, subs, cursor based on context, and so on. The proposed solutions were designed based on accessibility features already in use by other platforms.
</p><br />
<p><h4>ID: B19<br />
 Immersion at Scale: Researcher&#8217;s Guide to Ecologically Valid Mobile Experiments
</h4>
<div class="authors">
Soo Youn Oh,		Stanford University	<br />
Ketaki Shriram,	Stanford University	<br />
Bireswar Laha,	Stanford University	<br />
Shawnee Baughman,	Stanford University	<br />
Elise Ogle,		Stanford University	<br />
Jeremy Bailenson,	Stanford University<br />


<strong>Presenter</strong> Soo Youn Oh

</div><strong>Abstract: </strong> While there have been hundreds of psychological studies using virtual reality (VR) over the past few decades, those studies have almost exclusively been conducted in laboratory settings using small samples of college students with little demographic variance. Hence, the generalizability of the results is limited, as not all findings will apply outside the college demographic. In this paper, we present our mobile VR project (Immersion at Scale) where we conduct VR experiment sessions in naturalistic settings (e.g., local events, museums, etc.). On average, we were able to collect data from 20-25 people for each 4-hour data collection session of Immersion at Scale. We discovered a number of obstacles and opportunities based on bringing VR out into the field. Thus, we do not focus on experimental stimuli and results, but methodological guidelines based on our iterative design improvements from pilot testing.
</p><br />
<p><h4>ID: B20<br />
 Discovering Educational Augmented Reality Math Applications by Prototyping with Elementary-School Teachers
</h4>
<div class="authors">
Iulian Radu,		Georgia Institute of Technology	<br />
Betsy McCarthy,	WestEd	<br />
Yvonne Kao,		WestEd <br />
<strong>Presenter</strong> Iulian Radu

</div><strong>Abstract: </strong> In recent years, augmented reality (AR) applications for children’s entertainment have been gaining popularity, and educational organizations are increasingly interested in applying this technology to children’s educational games. In this paper we describe our collaboration with teachers and game designers, in order to explore educational potential for AR technology. This paper specifically investigates the topics of: What mathematics curriculum topics should technological innovations address in the Grade 1-3 classrooms? Which of the topics are suitable for AR games? And, how can we facilitate an efficient dialogue between educators and game designers?
</p><br />

<p><h4>ID: B21<br />
Improving the Curvature Manipulation Technique for Redirected Walking Using Passive Haptic Cues	
</h4>
<div class="authors">
Keigo Matsumoto,	Faculty of Engineering the University of Tokyo<br />
Yuki Ban,		Graduate School of Information Science and Technology, The University of Tokyo	<br />
Takuji Narumi,	Graduate School of Information Science and Technology, The University of Tokyo	<br />
Tomohiro Tanikawa,	Graduate School of Information Science and Technology, The University of Tokyo	<br />
Michitaka Hirose,	Graduate School of Information Science and Technology, The University of Tokyo <br />
								
<strong>Presenter</strong>	Keigo Matsumoto

</div><strong>Abstract: </strong> 	We propose a method for improving the effects of manipulation in redirected walking (RDW) by using passive haptic cues. In particular, we focus on a curvature manipulation technique and develop an RDW system, through which we can display a visual representation of a flat wall although, in reality, the user touches a curved surface wall. Using this system, we conduct an experiment to investigate the effects of our redirection techniques, and the results show that the proposed method using passive haptic cues can redirect users more effectively than conventional techniques that rely only on the visual manipulation.	
</p><br />


<p><h4>ID: B22<br />
 Mechanism of Inhibitory Effect of Cathodal Current Tongue Stimulation on Five Basic Tastes
</h4>
<div class="authors">
Satoru Sakurai,	Osaka University	<br />
Kazuma Aoyama,	Osaka University	<br />
Nobuhisa Miyamoto,	Osaka University	<br />
Makoto Mizukami,	Osaka University	<br />
Masahiro Furukawa,	Osaka University	<br />
Taro Maeda,		Osaka University <br />
Hideyuki Ando,	Osaka University <br />


<strong>Presenter</strong> Satoru Sakurai

</div><strong>Abstract: </strong> The mechanism by which cathodal current stimulation exerts inhibitory effects on the five basic tastes is revealed herein. The objective of this paper is to successfully achieve inhibition of sweetness by cathodal current electrical stimulation to the tongue, which has not been reported to date, though inhibition of salty and umami perception has been documented. By focusing on the electrophoresis of ions generated by dissolution of taste-inducing substances in water, this paper indicates how human gustation is inhibited by electrical stimulation, which is a key addition to the knowledgebase for achieving control of all five basic tastes.

</p><br />
<p><h4>ID: B23<br />
 Evaluating the Effects of Image Persistence on Dynamic Target Acquisition in Low Frame Rate Virtual Environments
</h4>
<div class="authors">
David Zielinski, 		Duke University	<br />
Hrishikesh Rao,		Duke University	<br />
Nick Potter, 			Duke University	<br />
Lawrence Appelbaum,	Duke University	<br />
Regis Kopper,			Duke University <br />

<strong>Presenter</strong> David J. Zielinski

</div><strong>Abstract: </strong> Here we explore a visual display technique for low frame rate virtual environments called low persistence (LP). This involves displaying the rendered frame for a single display frame and blanking the screen while waiting for the next frame to be generated. To gain greater knowledge about the LP technique, we have conducted a user study to evaluate user performance and learning during a dynamic target acquisition task. The task involved the acquisition of targets moving along several different trajectories, modeled after a shotgun trap shooting task. The results of our study indicate the LP condition approaches high frame rate performance within certain classes of target trajectories. Interestingly we also see that learning is consistent across conditions, indicating that it may not always be necessary to train under a visually high frame rate system.

</p><br /><p><h4>ID: B24<br />
 A Tour Guiding System of Historical Relics Based on Augmented Reality
</h4>
<div class="authors">
Wei Xiaodong,	Beijing Institute of Technology	<br />
	Weng Dongdong,	Beijing Institute of Technology	<br />
	Liu Yue,		Beijing Institute of Technology	<br />
	Wang Yongtian,	Beijing Institute of Technology <br />


<strong>Presenter</strong> Xiaodong Wei

</div><strong>Abstract: </strong> Yuanmingyuan is a relic park and only few cultural relics are left due to the looting and burning down in history, which makes that most of the scenic spots of the park look boring. To address such issue, a game-based guidance system for Yuanmingyuan and a time travel game called MAGIC-EYES has been proposed with Augmented Reality technology. Six interactive modes are designed in the proposed system to guide tourists to visit the specified place. The evaluation results of a pilot study shows that the proposed guidance system has significantly improved the tourist experiences.

</p><br />
<p><h4>ID: B25<br />
 Casting Shadows: Ecological Interface Design for Augmented Reality Pedestrian Collision Warning
</h4>
<div class="authors">
Hyungil Kim,		Virginia Tech	<br />
	Jessica Isleib,		Virginia Tech	<br />
	Joseph Gabbard,	Virginia Tech <br />


<strong>Presenter</strong> Hyungil Kim

</div><strong>Abstract: </strong> Ecological interface design (EID) has the opportunity to complement current approaches for augmented reality (AR) interface design by considering human-environment interaction and leveraging the inherent benefit of AR interfaces: conformal graphics. This work applies EID to design a novel interface for pedestrian collision warning for an automotive AR head-up display (HUD). Our initial usability evaluation shows potential benefits of incorporating EID into AR interface design.

</p><br />
<p><h4>ID: B26<br />
 The Effect of Realism on the Virtual Hand Illusion
</h4>
<div class="authors">
Lorraine Lin,		School of Computing, Clemson University	<br />
	Sophie	 Jörg,		School of Computing, Clemson University<br />


<strong>Presenter</strong> Lorraine Lin

</div><strong>Abstract: </strong> The virtual hand illusion is a body ownership illusion that occurs in a virtual environment. Previous studies reached different conclusions on the effect of realism of controllable virtual hand models on the intensity of the perceived illusion. We compare participants&#8217; responses to virtual impacts and threats when using hand models with different levels of realism. Our findings indicate that an illusion can occur for any model but that the effect is weakest for a non-anthropomorphic block model and strongest for a realistic human hand model in direct comparison. We furthermore find that reactions to our experiments highly vary between participants.

</p><br />
<p><h4>ID: B27<br />
 Evaluating two alternative walking in place interfaces for virtual reality gaming
</h4>
<div class="authors">
	Christian Toft,		Aalborg University	<br />
	Niels Nilsson,		Aalborg University	<br />
	Rolf Nordahl,		Aalborg University	<br />
	Stefania Serafin,	Aalborg University <br />


<strong>Presenter</strong> Rolf Nordahl

</div><strong>Abstract: </strong> This study investigates sliding as a walking-in-place (WIP) method for virtual reality navigation using the Wizdish. The Wizdish is a novel WIP device built for home usage. Two WIP methods, sliding and marching, were compared for naturalness, presence, and surface difference. The sliding technique used on the Wizdish was found to be significantly more disruptive during the experience compared to marching. This could be due to the size of the Wizdish, restricting the users stride, or due to a longer acclimatization time.

</p><br />
<p><h4>ID: B28<br />
 Disguising Rotational Gain for Redirected Walking in Virtual Reality: Effect of Visual Density
</h4>
<div class="authors">
Anders Paludan,	Aalborg University	<br />
	Niels Nilsson,		Aalborg University <br />	
	Rolf Nordahl,		Aalborg University	<br />
	Stefania Serafin,	Aalborg University <br />


<strong>Presenter</strong> Rolf Nordahl

</div><strong>Abstract: </strong> In virtual reality environments that allow users to walk freely, the area of the virtual environment (VE) is constrained to the size of the tracking area. By using redirection techniques, this problem can be partially circumvented; one of the techniques involves rotating the user more or less in the virtual world than in the physical world; this technique is referred to as rotational gain. This paper seeks to further investigate this area, examining the effect of visual density in the VE.

</p><br />
<p><h4>ID: B29<br />
 De-escalation Training in an Augmented Virtuality Space
</h4>
<div class="authors">
Charles Hughes,	University of Central Florida	<br />
	Kathleen Ingraham,	University of Central Florida <br />


<strong>Presenter</strong> Charlie Hughes

</div><strong>Abstract: </strong> This poster describes the TeachLivE paradigm, its application to de-escalation training, especially for law enforcement personnel, and the realization of this in a four-walled augmented virtuality space. Emphasis is placed on how the resulting physical presence increases co-presence and social presence, leading to an immersive and effective learning environment.

</p><br />
<p><h4>ID: B30<br />
 FaceBo: Facial and Body Tracking for Faithful Synthesis of Avatar
</h4>
<div class="authors">
Jean-Luc Lugrin,	University of Würzburg	<br />
	Daniel Roth,		University of Cologne	<br />
	David Zilch,		University of Würzburg	<br />
	Gary Bente,		University of Cologne	<br />
	Marc Erich Latoschik,	University of Würzburg <br />


<strong>Presenter</strong> Marc Erich Latoschik

</div><strong>Abstract: </strong> This paper introduces a low-cost framework capable of combining both real-time markerless face and body tracking for faithful avatar embodiment in Virtual Reality (VR). We discuss suitable hardware and software solutions and present a first prototype. This work lays the technological basis for further research on the importance of the appearance and behavioral realism of avatars, e.g., for the illusion of virtual body ownership, for social interactions in VR, as well as for VR entertainment applications (immersive games or movies).

</p><br />
<p><h4>ID: B31<br />
 An Intelligent Multimodal Mixed Reality Real-Time Strategy Game
</h4>
<div class="authors">
Sascha Link,		University of Würzburg	<br />
	Berit Barkschat,	University of Würzburg	<br />
	Chris Zimmerer,	University of Würzburg	<br />
	Martin Fischbach,	University of Würzburg	<br />
	Dennis Wiebusch,	University of Würzburg	<br />
	Jean-Luc Lugrin,	University of Würzburg	<br />
	Marc Erich Latoschik,	University of Würzburg<br />


<strong>Presenter</strong> Dennis Wiebusch

</div><strong>Abstract: </strong> This paper presents a mixed reality tabletop role-playing game with a novel combination of interaction styles and gameplay mechanics. Our contribution extends previous approaches by abandoning the traditional turn-based gameplay in favor of simultaneous real-time interaction. The increased cognitive and physical load during the simultaneous control of multiple game characters is counteracted by two features: First, certain game characters are equipped with AI-driven capabilities to become semi-autonomous virtual agents. Second, (groups of) these agents can be instructed by high-level commands via a multimodal -speech and gesture- interface.

</p><br />
<p><h4>ID: B32<br />
 Redirected Head Gaze to Support AR Meetings Distributed Over Heterogeneous Environments
</h4>
<div class="authors">
Taeheon Kim,		Georgia Institute of Technology	<br />
	Ashwin Kachhara,	Georgia Institute of Technology	<br />
	Blair MacIntyre,	Georgia Institute of Technology <br />


<strong>Presenter</strong> Taeheon Kim

</div><strong>Abstract: </strong> We demonstrate a method for redirecting gaze of virtual avatars in distributed augmented reality (AR) meetings. As social cues are a necessity for effective communication, our method tries to preserve gaze awareness, one of the key elements of a face-to-face meeting. When using AR to bring multiple sites together in a distributed meeting, with different numbers of participants and physical arrangements across sites, gaze awareness is maintained regardless of the seating topology. By maintaining gaze, we hope to enhance the presence of remote attendees and improve communication among the users, making meetings in AR a practical option for teleconferencing.

</p><br />
<p><h4>ID: B33<br />
 Integrating Videos with LIDAR Scans for Virtual Reality
</h4>
<div class="authors">
Giang Bui,		University of Missouri	<br />
	Brittany Morago,	University of Missouri	<br />
	Truc Le,		University of Missouri	<br />
	Kevin Karsch,		University of Missouri	<br />
	Zheyu Lu,		University of Missouri	<br />
	Ye Duan,		University of Missouri<br />


<strong>Presenter</strong> Ye Duan

</div><strong>Abstract: </strong> LIDAR range scans can be used to quickly create accurate 3D models for virtual reality and as a basis to visualize sets of photographs, videos, and virtual objects in a cohesive environment. We demonstrate how to register a variety of 2D imagery with a range scan to construct photo-realistic models and to extract walking people captured in videos and model them in a 3D space. We also present a method for determining the sun position from a set of stitched photographs in order to apply correct lighting to virtual objects placed amongst real world data.

</p><br />
<p><h4>ID: B34<br />
 Virtual Energy Center for Teaching Alternative Energy Technologies
</h4>
<div class="authors">
Christoph Borst,	University of Louisiana at Lafayette	<br />
	Kary Ritter,		University of Louisiana at Lafayette	<br />
	Terrence Chambers,	University of Louisiana at Lafayette	<br />


<strong>Presenter</strong> Christoph W. Borst

</div><strong>Abstract: </strong> We overview the Virtual Energy Center, a VR environment that models a real energy facility to enable virtual field trips and self-guided exploration. VEC is augmented by visual guides and educational content to teach students about concentrating solar power technology. A teacher physically near the student can appear in the scene via depth camera imagery, allowing the teacher to walk around in a classroom setting and assist students. Work-in-progress is streaming the depth images over a network to allow students to virtually meet expert guides from the real facility. We summarize these features, some interaction-related challenges, and ongoing testing.

</p><br /><p><h4>ID: B35<br />
 Simultaneous Mapping and Redirected Walking for ad hoc Free Walking in Virtual Environments
</h4>
<div class="authors">
Thomas Nescher,	ETH Zurich	<br />
	Markuz Zank,		ETH Zurich	<br />
	Andreas Kunz,	ETH Zurich <br />


<strong>Presenter</strong> Thomas Nescher

</div><strong>Abstract: </strong> This paper presents an approach that combines redirected walking with a low-cost and user-worn tracking approach based on simultaneous localization and mapping (SLAM). I.e. learning the environment with the walkable area, tracking the user&#8217;s viewpoint, and redirected walking is done on the fly without any prior setup, without preparing a room, and without setting up a tracking system. This allows ad hoc free walking in virtual environments even within dynamic and cluttered physical rooms, where the walkable area is of arbitrary shape.

</p><br />
<p><h4>ID: B36<br />
 Perceptual Space Warping: Preliminary Exploration
</h4>
<div class="authors">
Alex Peer,	University of Wisconsin-Madison	<br />
	Kevin Ponto,	University of Wisconsin-Madison <br />


<strong>Presenter</strong> Alex Peer

</div><strong>Abstract: </strong> Distance has been shown to be incorrectly estimated in virtual environments relative to the same estimation tasks in a real environment. This work describes a preliminary exploration of Perceptual Space Warping, which influences perceived distance in virtual environments by using a vertex shader to warp geometry. Empirical tests demonstrate significant effects, but of smaller magnitude than expected. This raises further questions about the complex interactions between the presentation and perception of space in a virtual environment.

</p><br />
<p><h4>ID: B37<br />
 Evaluation of the Effect of a Virtual Avatar&#8217;s Representation on Distance Perception in Immersive Virtual Environments
</h4>
<div class="authors">
Dimitar Valkov,	University of Münster	<br />
	John Martens,		University of Münster	<br />
	Klaus Hinrichs,	University of Münster<br />


<strong>Presenter</strong> Dimitar Valkov

</div><strong>Abstract: </strong> It is well known that distance estimation in IVEs suffers from compression when viewed from an egocentric perspective with a HMD. While previous research indicates that providing the user with an avatar with high geometric and motion fidelity may alleviate this problem, little work has been done to investigate which properties of the avatar’s representation influence the distance estimation. In this poster we report the results of an evaluation of the user’s distance perception with different avatar representations. Our results indicate that anthropometric fidelity of the avatar has stronger effect on the distance perception than its visual fidelity.

</p><br />
<p><h4>ID: B38<br />
 Depth-based 3D Gesture Multi-Level Radial Menu for Virtual Object Manipulation
</h4>
<div class="authors">
	Matthew M. Davis,	Virginia Tech	<br />
	Joseph L. Gabbard,	Virginia Tech	<br />
	Doug A. Bowman,	Virginia Tech	<br />
	Dennis Gracanin,	Virginia Tech	<br />


<strong>Presenter</strong> Matthew M. Davis

</div><strong>Abstract: </strong> In this work, we present a depth-based solution to multi-level menus for selection and manipulation of virtual objects using free-hand gestures. Navigation between and through menus is performed using three gesture states that utilize X, Y translations of the finger with boundary crossing. Although presented in a single context, this menu structure can be applied to a myriad of domains requiring several levels of menu data, and serves to supplement existing and emerging menu design for augmented, virtual, and mixed-reality applications.

</p><br />
<p><h4>ID: B39<br />
 Progressive Feedback Point Cloud Rendering for Virtual Reality Display
</h4>
<div class="authors">
Ross Tredinnick,	University of Wisconsin &#8211; Madison	<br />
	Kevin Ponto,		University of Wisconsin &#8211; Madison	<br />
	Markus Broecker,	University of Wisconsin &#8211; Madison <br />

<strong>Presenter</strong> Kevin Ponto

</div><strong>Abstract: </strong> Previous approaches to rendering large point clouds on immersive displays have generally created a trade-off between interactivity and quality. While these approaches have been quite successful for desktop environments when interaction is limited, virtual reality systems are continuously interactive, which forces users to suffer through either low frame rates or low image quality. This paper presents a novel approach to this problem through a progressive feedback-driven rendering algorithm. This algorithm uses reprojections of past views to accelerate the reconstruction of the current view. The presented method is tested against previous methods, showing improvements in both rendering quality and interactivity.

</p><br />
<p><h4>ID: B40<br />
 Using Projection AR to Add Design Studio Pedagogy to a CS Classroom
</h4>
<div class="authors">
Blair MacIntyre,	Georgia Institute of Technology	<br />
	Dingtian Zhang,	Georgia Institute of Technology	<br />
	Ryan Jones,		Georgia Institute of Technology	<br />
	Amber Solomon,	Georgia Institute of Technology	<br />
	Elizabeth DiSalvo,	Georgia Institute of Technology	<br />
	Mark Guzdial,		Georgia Institute of Technology <br />


<strong>Presenter</strong> Ryan Jones

</div><strong>Abstract: </strong> We use projection augmented reality to add design studio learning models to a classroom for an introductory Media Computation computer science class. Students do classwork using an enhanced version of Pythy that captures students’ work and displays it around the room. We leverage the Microsoft RoomAlive Toolkit to construct a room-scale augmented reality. The system “pins” students’ work to the walls, where teachers and students can see and discuss the work. We hope that the system will foster collaboration and support creating STEM learning experiences that encourage creativity, and help build strong peer learning environment.



</p><br />
<p><h4>ID: B41<br />
A Low-cost, Low-latency Approach to Dynamic Immersion in Occlusive Head-Mounted Displays
</h4>
<div class="authors">
	Robert Lindeman,	HIT Lab NZ, University of Canterbury	<br />

<strong>Presenter</strong>	Rob Lindeman

</div><strong>Abstract: </strong>	We introduce a method for dynamically controlling the level of immersion provided by HMDs. We replace the cowling around typical ski-goggle type HMDs with LCD panels whose transparency can be controlled using very simple stand-alone circuitry or a micro-controller to vary the amount of the real world that is visible in the periphery of the user. This allows users to see objects in their immediate surroundings (e.g., the keyboard and mouse), can be used to counter cybersickness by providing natural cues, and introduces no added latency into the system.


</p><br />
<p><h4>ID: B42<br />
A Realistic Walking Model for Enhancing Redirection in Virtual Reality
</h4>
<div class="authors">
	Courtney Hutton,	Occidental College	<br />
	Evan Suma,		USC Institute for Creative Technologies	<br />

<strong>Presenter</strong>	Courtney Hutton

</div><strong>Abstract: </strong>	Redirected walking algorithms require the prediction of human motion in order to effectively steer users away from the boundaries of the physical space.  While a virtual walking trajectory may be represented using straight lines connecting waypoints of interest, this simple model does not accurately represent typical user behavior.  In this poster, we present a more realistic walking model for use in real-time virtual environments that employ redirection techniques.  We implemented the model within a framework that can be used for simulation of redirected walking within different virtual and physical environments. 
</p><br />
																	</div>
									</article>
							
							</div>
														<footer class="site-info" itemscope itemtype="http://schema.org/WPFooter">
										© 2015-2016 IEEE Virtual Reality.
				</footer>
														</div>
					</div>
	</div>
</div>
        <div id="fb-root"></div>
		<script>(function(d, s, id) {
		  var js, fjs = d.getElementsByTagName(s)[0];
		  if (d.getElementById(id)) return;
		  js = d.createElement(s); js.id = id;
		  js.src = "../../../connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3";
		  fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));</script>
		<!--facebook like and share js -->                   
        <!--<div id="fb-root"></div>
        <script>
        (function(d, s, id) {
          var js, fjs = d.getElementsByTagName(s)[0];
          if (d.getElementById(id)) return;
          js = d.createElement(s); js.id = id;
          js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&appId=1425108201100352&version=v2.0";
          fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));</script>-->
 	         <!--google share and  like and e js -->
        <script type="text/javascript">
            window.___gcfg = {
              lang: 'en-US'
            };
            (function() {
                var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
                po.src = '../../../apis.google.com/js/plusone.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
            })();
        </script>
		<script type='text/javascript' src='../../../apis.google.com/js/plusone.js'></script>
        <script type='text/javascript' src='../../../apis.google.com/js/platform.js'></script>
        <!-- google share -->
        <script type="text/javascript">
          (function() {
            var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
            po.src = '../../../apis.google.com/js/platform.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
          })();
        </script>
			<!-- twitter JS End -->
		<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="../../../platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>	
	     <script>
	    jQuery( document ).scroll(function( $ )
		{
	    	var y = jQuery(this).scrollTop();
	      	if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent))
			{	 
			   if(jQuery(window).scrollTop() + jQuery(window).height() >= jQuery(document).height()-100)
			   {
				  jQuery('.sfsi_outr_div').css({'z-index':'9996',opacity:1,top:jQuery(window).scrollTop()+"px",position:"absolute"});
				  jQuery('.sfsi_outr_div').fadeIn(200);
				  jQuery('.sfsi_FrntInner_chg').fadeIn(200);
			   }
			   else{
				   jQuery('.sfsi_outr_div').fadeOut();
				   jQuery('.sfsi_FrntInner_chg').fadeOut();
			   }
		  }
		  else
		  {
			   if(jQuery(window).scrollTop() + jQuery(window).height() >= jQuery(document).height()-3)
			   {
					jQuery('.sfsi_outr_div').css({'z-index':'9996',opacity:1,top:jQuery(window).scrollTop()+200+"px",position:"absolute"});
					jQuery('.sfsi_outr_div').fadeIn(200);
					jQuery('.sfsi_FrntInner_chg').fadeIn(200);
		  	   }
	 		   else
			   {
				 jQuery('.sfsi_outr_div').fadeOut();
				 jQuery('.sfsi_FrntInner_chg').fadeOut();
			   }
	 	  } 
		});
     </script>
     <script src='../wp-content/plugins/ultimate-social-media-icons/js/jquery-migrate-min3a05.js?ver=4.2.2'></script>
<script src='../wp-content/plugins/ultimate-social-media-icons/js/jquery-ui-min3a05.js?ver=4.2.2'></script>
<script src='../wp-content/plugins/ultimate-social-media-icons/js/shuffle/modernizr.custom.min3a05.js?ver=4.2.2'></script>
<script src='../wp-content/plugins/ultimate-social-media-icons/js/shuffle/jquery.shuffle.min3a05.js?ver=4.2.2'></script>
<script src='../wp-content/plugins/ultimate-social-media-icons/js/shuffle/random-shuffle-min3a05.js?ver=4.2.2'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ajax_object = {"ajax_url":"http:\/\/ieeevr.org\/2016\/wp-admin\/admin-ajax.php"};
var ajax_object = {"ajax_url":"http:\/\/ieeevr.org\/2016\/wp-admin\/admin-ajax.php","plugin_url":"http:\/\/ieeevr.org\/2016\/wp-content\/plugins\/ultimate-social-media-icons\/"};
/* ]]> */
</script>
<script src='../wp-content/plugins/ultimate-social-media-icons/js/custom3a05.js?ver=4.2.2'></script>
</body>

<!-- Mirrored from ieeevr.org/2016/posters/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 22 May 2020 15:39:14 GMT -->
</html>