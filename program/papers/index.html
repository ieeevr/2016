<!DOCTYPE html>
<html lang="en-US">

<!-- Mirrored from ieeevr.org/2016/program/papers/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 22 May 2020 15:37:46 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
		<meta charset="UTF-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<link rel="profile" href="http://gmpg.org/xfn/11" />
	<link rel="pingback" href="#" />
	<title>Papers | IEEE VR 2016</title>
<link rel="alternate" type="application/rss+xml" title="IEEE VR 2016 &raquo; Feed" href="../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="IEEE VR 2016 &raquo; Comments Feed" href="../../comments/feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="IEEE VR 2016 &raquo; Papers Comments Feed" href="feed/index.html" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"http:\/\/s.w.org\/images\/core\/emoji\/72x72\/","ext":".png","source":{"concatemoji":"http:\/\/ieeevr.org\/2016\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.2.2"}};
			!function(a,b,c){function d(a){var c=b.createElement("canvas"),d=c.getContext&&c.getContext("2d");return d&&d.fillText?(d.textBaseline="top",d.font="600 32px Arial","flag"===a?(d.fillText(String.fromCharCode(55356,56812,55356,56807),0,0),c.toDataURL().length>3e3):(d.fillText(String.fromCharCode(55357,56835),0,0),0!==d.getImageData(16,16,1,1).data[0])):!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g;c.supports={simple:d("simple"),flag:d("flag")},c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.simple&&c.supports.flag||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='SFSImainCss-css' href='../../wp-content/plugins/ultimate-social-media-icons/css/sfsi-style3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='SFSIJqueryCSS-css' href='../../wp-content/plugins/ultimate-social-media-icons/css/jquery-ui-1.10.4/jquery-ui-min3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='flat-fonts-css' href='http://fonts.googleapis.com/css?family=Amatic+SC%7CRoboto:400,700%7CRoboto+Slab%7CRoboto+Condensed' media='all' />
<link rel='stylesheet' id='flat-theme-css' href='../../wp-content/themes/flat/assets/css/flat.minba3a.css?ver=1.7.2' media='all' />
<link rel='stylesheet' id='flat-style-css' href='../../wp-content/themes/flat/style3a05.css?ver=4.2.2' media='all' />
<link rel='stylesheet' id='sccss_style-css' href='../../index89d4.html?sccss=1&amp;ver=4.2.2' media='all' />
<script src='../../wp-includes/js/jquery/jquery4a80.js?ver=1.11.2'></script>
<script src='../../wp-includes/js/jquery/jquery-migrate.min1576.js?ver=1.2.1'></script>
<script src='../../wp-content/themes/flat/assets/js/flat.minba3a.js?ver=1.7.2'></script>
<!--[if lt IE 9]>
<script src='http://ieeevr.org/2016/wp-content/themes/flat/assets/js/html5shiv.min.js?ver=3.7.2'></script>
<![endif]-->
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../xmlrpc0db0.html?rsd" />
<meta name="generator" content="WordPress 4.2.2" />
<link rel='canonical' href='index.html' />
<link rel='shortlink' href='../../indexba52.html?p=39' />
<meta name="viewport" content="width=device-width, initial-scale=1"><link type="image/x-icon" href="../../wp-content/uploads/2015/07/Favicon.png" rel="shortcut icon"><style type="text/css">#page:before, .sidebar-offcanvas, #secondary { background-color: #f66733; }@media (max-width: 1199px) { #page &gt; .container { background-color: #f66733; } }body { background-size: contain; }</style><style type="text/css">#masthead .site-title {font-family:Amatic SC}body {font-family:Roboto }h1,h2,h3,h4,h5,h6 {font-family:Roboto Slab}#masthead .site-description, .hentry .entry-meta {font-family:Roboto Condensed}</style><style type="text/css" id="custom-background-css">
body.custom-background { background-image: url('../../wp-content/uploads/2015/07/VRBackgroundAlt.png'); background-repeat: repeat-y; background-position: top center; background-attachment: fixed; }
</style>
	</head>

<body class="page page-id-39 page-child parent-pageid-31 page-template-default custom-background" itemscope itemtype="http://schema.org/WebPage">
<div id="page">
<div class="topbanner">
<img class="topbannerimg" src="../../wp-content/uploads/2016/01/Banner-w-Logos.png" alt="VR Banner with Peaches" width="100%">
</div>
	<div class="container">
		<div class="row row-offcanvas row-offcanvas-left">
			<div id="secondary" class="col-lg-3">
								<header id="masthead" class="site-header" role="banner">
										<div class="hgroup">
						<h1 class="site-title display-logo"><a href="../../index.html" title="IEEE VR 2016" rel="home"><img itemprop="primaryImageofPage" alt="IEEE VR 2016" src="../../wp-content/uploads/2015/07/IEEE2016-round-whitetext-01.png" /></a></h1><h2 itemprop="description" class="site-description">March 19 – 23, 2016</h2>					</div>
					<button type="button" class="btn btn-link hidden-lg toggle-sidebar" data-toggle="offcanvas" aria-label="Sidebar"><i class="fa fa-gear"> Sponsors</i></button>
					<button type="button" class="btn btn-link hidden-lg toggle-navigation" aria-label="Navigation Menu">Menu  <i class="fa fa-bars"></i></button>
					<nav id="site-navigation" class="navigation main-navigation" role="navigation">
						<ul id="menu-main-menu" class="nav-menu"><li id="menu-item-167" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-167"><a href="../../index.html">Home</a></li>
<li id="menu-item-482" class="menu-item menu-item-type-custom menu-item-object-custom current-menu-ancestor current-menu-parent menu-item-has-children menu-item-482"><a href="../program-overview/index.html">Program</a>
<ul class="sub-menu">
	<li id="menu-item-219" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-219"><a href="../program-overview/index.html">Program Overview</a></li>
	<li id="menu-item-214" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-214"><a href="../keynote/index.html">Keynote</a></li>
	<li id="menu-item-672" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-672"><a href="../capstone-presentation/index.html">Capstone Presentation</a></li>
	<li id="menu-item-217" class="menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-39 current_page_item menu-item-217"><a href="index.html">Papers</a></li>
	<li id="menu-item-871" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-871"><a href="../../posters/index.html">Posters</a></li>
	<li id="menu-item-216" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-216"><a href="../panels/index.html">Panels</a></li>
	<li id="menu-item-223" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-223"><a href="../workshop-papers/index.html">Workshops</a>
	<ul class="sub-menu">
		<li id="menu-item-427" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-427"><a href="../workshop-papers/third-ieee-vr-international-workshop-on-collaborative-virtual-environments-3dcve/index.html">Collaborative Virtual Environments (3DCVE)</a></li>
		<li id="menu-item-419" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-419"><a href="../workshop-papers/second-workshop-on-everyday-virtual-reality/index.html">Everyday Virtual Reality</a></li>
		<li id="menu-item-395" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-395"><a href="../workshop-papers/ieee-vr-2016-workshop-on-perceptual-and-cognitive-issues-in-ar-percar/index.html">Perceptual and Cognitive Issues in AR</a></li>
		<li id="menu-item-415" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-415"><a href="../workshop-papers/immersive-analytics-ia/index.html">Immersive Analytics (IA)</a></li>
		<li id="menu-item-416" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-416"><a href="../workshop-papers/mixed-reality-art-mra-workshop/index.html">Mixed Reality Art (MRA)</a></li>
		<li id="menu-item-468" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-468"><a href="../workshop-papers/ieee-virtual-reality-2016-workshop-on-k-12-embodied-learning-through-virtual-augmented-reality-kelvar/index.html">K-12 Embodied Learning through Virtual &#038; Augmented Reality (KELVAR)</a></li>
		<li id="menu-item-417" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-417"><a href="../workshop-papers/9th-workshop-on-software-engineering-and-architectures-for-realtime-interactive-systems/index.html">Software Engineering and Architectures for Realtime Interactive Systems</a></li>
		<li id="menu-item-418" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-418"><a href="../workshop-papers/ieee-vr-workshop-on-virtual-humans-and-crowds-for-immersive-environments/index.html">Virtual Humans and Crowds for Immersive Environments</a></li>
	</ul>
</li>
	<li id="menu-item-221" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-221"><a href="../tutorials/index.html">Tutorials</a></li>
	<li id="menu-item-220" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-220"><a href="../research-demos/index.html">Research Demos</a></li>
	<li id="menu-item-222" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-222"><a href="../videos/index.html">Videos</a></li>
	<li id="menu-item-212" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-212"><a href="../exhibitors/index.html">Exhibitors</a></li>
</ul>
</li>
<li id="menu-item-169" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-169"><a href="../../attend/registration/index.html">Attend</a>
<ul class="sub-menu">
	<li id="menu-item-537" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-537"><a href="../../attend/registration/index.html">Registration</a></li>
	<li id="menu-item-239" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-239"><a href="../../attend/venue/index.html">Venue</a></li>
	<li id="menu-item-236" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-236"><a href="../../attend/accommodations/index.html">Accommodations</a></li>
	<li id="menu-item-238" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-238"><a href="../../attend/transportion/index.html">Transportation</a></li>
	<li id="menu-item-1038" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1038"><a href="../../attend/restaurants-2/index.html">Restaurants</a></li>
	<li id="menu-item-1039" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1039"><a href="../../attend/attractions-2/index.html">Attractions</a></li>
</ul>
</li>
<li id="menu-item-168" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-168"><a href="../../contribute/papers/index.html">Contribute</a>
<ul class="sub-menu">
	<li id="menu-item-362" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-362"><a href="../../contribute/call-for-doctoral-consortium/index.html">Doctoral Consortium</a></li>
	<li id="menu-item-225" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-225"><a href="../../contribute/exhibitors-and-supporters/index.html">Exhibitors and Supporters</a></li>
	<li id="menu-item-338" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-338"><a href="../../contribute/call-for-panels/index.html">Panels</a></li>
	<li id="menu-item-229" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-229"><a href="../../contribute/papers/index.html">Papers</a></li>
	<li id="menu-item-321" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-321"><a href="../../contribute/call-for-posters/index.html">Posters</a></li>
	<li id="menu-item-381" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-381"><a href="../../contribute/call-for-research-demos/index.html">Research Demos</a></li>
	<li id="menu-item-388" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-388"><a href="../../contribute/call-for-student-volunteers/index.html">Student Volunteers</a></li>
	<li id="menu-item-354" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-354"><a href="../../contribute/call-for-tutorials/index.html">Tutorials</a></li>
	<li id="menu-item-399" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-399"><a href="../../contribute/call-for-videos/index.html">Videos</a></li>
	<li id="menu-item-528" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-528"><a href="../../contribute/call-for-workshops/workshops/index.html">Workshops</a></li>
</ul>
</li>
<li id="menu-item-1052" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1052"><a href="../../participate/presenter-instructions/index.html">Presenter Instructions</a></li>
<li id="menu-item-170" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-170"><a href="../../committees/conference-committee/index.html">Committees</a>
<ul class="sub-menu">
	<li id="menu-item-240" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-240"><a href="../../committees/conference-committee/index.html">Conference Committee</a></li>
	<li id="menu-item-241" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-241"><a href="../../committees/program-committee/index.html">Program Committee</a></li>
	<li id="menu-item-242" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-242"><a href="../../committees/steering-committee/index.html">Steering Committee</a></li>
</ul>
</li>
<li id="menu-item-1090" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1090"><a href="../../awards-2/index.html">Awards</a></li>
<li id="menu-item-243" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-243"><a href="../../past-conferences/index.html">Past Conferences</a></li>
<li id="menu-item-171" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-171"><a target="_blank" href="http://3dui.org/">IEEE 3DUI 2016</a></li>
<li id="menu-item-1037" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1037"><a href="../../posters/index.html">Posters</a></li>
</ul>					</nav>
									</header>
				
				<div class="sidebar-offcanvas">
					<div id="main-sidebar" class="widget-area" role="complementary">
											<aside id="sfsi-widget-3" class="widget sfsi">
            <div class="sfsi_widget" data-position="widget">   
				<div id='sfsi_wDiv'></div>
                    <div class="norm_row sfsi_wDiv"  style="width:225px;text-align:left;position:absolute;"><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale'   href='javascript:void(0)' id='sfsiid_facebook' alt='Facebook' style='opacity:1' ><img alt='Facebook' title='Facebook' src='../../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_facebook.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a><div class="sfsi_tool_tip_2 fb_tool_bdr sfsiTlleft" style="width:62px ;opacity:0;z-index:-1;margin-left:-47.5px;" id="sfsiid_facebook"><span class="bot_arow bot_fb_arow"></span><div class="sfsi_inside"><div  class='icon1'><a href='https://www.facebook.com/ieeevr' target='_blank'><img alt='Facebook' title='Facebook' src='../../wp-content/plugins/ultimate-social-media-icons/images/visit_icons/facebook.png'  /></a></div><div  class='icon2'><div class="fb-like" data-href="http://ieeevr.org/2016/program/papers/" data-layout="button" data-action="like" data-show-faces="false" data-share="true"></div></div></div></div></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.twitter.com/IEEEVR' id='sfsiid_twitter' alt='Twitter' style='opacity:1' ><img alt='Twitter' title='Twitter' src='../../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_twitter.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.youtube.com/user/ieeevrconf' id='sfsiid_youtube' alt='YouTube' style='opacity:1' ><img alt='YouTube' title='YouTube' src='../../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_youtube.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div><div style='width:40px; height:40px;margin-left:5px;margin-bottom:5px;' class='sfsi_wicons shuffeldiv '><div class='inerCnt'><a class=' sficn' effect='scale' target='_blank'  href='http://www.specificfeeds.com/widget/emailsubscribe/MTU5MDQx/OA==/' id='sfsiid_email' alt='Email general chairs' style='opacity:1' ><img alt='Email general chairs' title='Email general chairs' src='../../wp-content/plugins/ultimate-social-media-icons/images/icons_theme/flat/flat_email.png' width='40' style='' class='sfcm sfsi_wicon' effect='scale'   /></a></div></div></div ><div id="sfsi_holder" class="sfsi_holders" style="position: relative; float: left;width:100%;z-index:-1;"></div ><script>jQuery(".sfsi_widget").each(function( index ) {
					if(jQuery(this).attr("data-position") == "widget")
					{
						var wdgt_hght = jQuery(this).children(".norm_row.sfsi_wDiv").height();
						var title_hght = jQuery(this).parent(".widget.sfsi").children(".widget-title").height();
						var totl_hght = parseInt( title_hght ) + parseInt( wdgt_hght );
						jQuery(this).parent(".widget.sfsi").css("min-height", totl_hght+"px");
					}
				});</script>	      		<div style="clear: both;"></div>
            </div>
            					</aside>
					<aside id="text-6" class="widget widget_text">
						<h3 class='widget-title'>Exhibitors and Supporters</h3>
			<div class="textwidget"><h6>Diamond Level</h6>
<center>
<img src="../../wp-content/uploads/2015/10/NSF.png" width="80%" align="middle"  style="background-color: white;">

<img src="../../wp-content/uploads/2016/02/ClemsonCES.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/SOCNewLogo.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Silver Level</h6>
<center>
<img src="../../wp-content/uploads/2016/02/ART-logo-black_300dpi_A4.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/coeLogo.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Bronze Level</h6>
<center>
<img src="../../wp-content/uploads/2016/02/DP001.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/esi_full-colour_tagline_rgb.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/logo_big.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/MiddleVR-ImprooveReality.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/Noraxon-Logo-Transparent-Bkgrd.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/Polhemus-Green_black-tag.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/Vicon-Logo.png" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/02/Black-circle_Grey-orld_i_Black-viz_tagline.png" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center>

<h6>Publisher</h6>
<center>
<img src="../../wp-content/uploads/2016/02/Presence-e-logo-150x150.gif" width="80%" align="middle"  style="background-color: white;" vspace="20">

<img src="../../wp-content/uploads/2016/03/MC_logo_color_square.jpg" width="80%" align="middle"  style="background-color: white;" vspace="20">
</center></div>
							</aside>
											</div>
				</div>
			</div>

						<div id="primary" class="content-area col-lg-9" itemprop="mainContentOfPage">
										<div itemscope itemtype="http://schema.org/Article" id="content" class="site-content" role="main">
				
							<article id="post-39" class="post-39 page type-page status-publish hentry">
					<header class="entry-header">
						<h1 class="entry-title" itemprop="name">Papers</h1>
					</header>
										<div class="entry-content" itemprop="articleBody">
												<div class="table-responsive" id="paperOverview">
    <table>
        <tr>
            <td class="day"><a href="#monday"><h4>Monday<br />March 21</h4></a></td>
            <td class="day"><a href="#tuesday"><h4>Tuesday<br />March 22</h4></a></td>
            <td class="day"><a href="#wednesday"><h4>Wednesday<br />March 23</h4></a></td>
        </tr>
        <tr>
            <td class="orange">
                <ul>
                    <li><a href="#humans">10:15 am – 12:30 pm<br />Virtual Humans and Crowds</a></li>
                    <li><a href="#earsHands">3:30 pm – 5:00 pm<br />Ears and Hands</a></li>
                    <li><a href="#cga">3:30 pm – 5:00 pm<br />IEEE Computer Graphics &#038; Applications</a></li>
                </ul>
            </td>
            <td class="purple">
                <ul>
                    <li><a href="#system">8:30 am – 10:00 am<br />Systems and Latency</a></li>
                    <li><a href="#perception">10:15 am – 12:30 pm<br />Perception and Cognition</a></li>
                    <li><a href="#arCalibration">1:45 pm – 3:15 pm<br />Augmented Reality and Calibration</a></li>
                    <li><a href="#displays">3:30 pm – 5:00 pm<br />Displays and Sensory Integration</a></li>
                </ul>
            </td>
            <td class="orange">
                <ul>
                    <li><a href="#interaction">10:00 am – 12:30 pm<br />Interaction and Immersion</a></li>
                    <li><a href="#applications">1:45 pm – 3:15 pm<br />Applications</a></li>
                </ul>
            </td>
        </tr>
    </table>
</div>

<div id="monday">Monday, 21st March 2016</div>
<h3 class="catTitle" id="humans">10:15 am &#8211; 12:30 pm<br />Virtual Humans and Crowds</h3>
<strong>Session Chair: Anne-Hélène Olivier</strong>
<p><h4>Effects of virtual human appearance fidelity on emotion contagion in affective inter-personal simulations</h4>
TVCG<br />
<div class="authors">Matias Volonte, Himanshu Chaturvedi, Nathan Newsome, Elham Ebrahimi, Tania Roy, Shaundra Daily, Tracy Fasolino, Sabarish V. Babu</div>
<img src="../../wp-content/uploads/2016/02/teaser.png"><strong>Abstract: </strong>Realistic versus stylized depictions of virtual humans in simulated inter-personal situations and their ability to elicit emotional responses in users has been an open question for artists and researchers alike. We empirically evaluated the effects of realistic vs. non-realistic stylized appearance of virtual humans on the emotional response of participants in a medical virtual reality system that was designed to educate users in recognizing the signs and symptoms of patient deterioration. In social emotional constructs of shyness, presence, perceived personality, and enjoyment-joy, we found that participants responded differently in the visually realistic condition as compared to cartoon and sketch conditions.
</p>
<br />
<p><h4>The Role of Interaction in Virtual Embodiment : Effects of the Virtual Hand Representation</h4>
IEEE VR proceedings<br />
<div class="authors">Ferran Argelaguet Sanz, Ludovic Hoyet, Michaël Trico, Anatole Lécuyer</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0233-file3.jpg"></div><strong>Abstract: </strong>How do people appropriate their virtual hand representation when interacting in virtual environments? In order to answer this question, we conducted an experiment studying the sense of embodiment when interacting with three different virtual hand representations, each one providing a different degree of visual realism but keeping the same control mechanism. The main experimental task was a Pick-and-Place task in which participants had to grasp a virtual cube and place it to an indicated position while avoiding an obstacle (brick, barbed wire or fire). An additional task was considered in which participants had to perform a potentially endangering operation towards their virtual hand: place their virtual hand close to a virtual spinning saw. Both qualitative measures and questionnaire data were gathered in order to assess the sense of agency and ownership towards each virtual hand. Results show that the sense of agency is stronger for less realistic virtual hands which also provide less  mismatch between the participant&#8217;s actions and the animation of the virtual hand. In contrast, the sense of ownership is increased for the human virtual hand which provides a direct mapping between the degrees of freedom of the real and virtual hand.
</p>
<br />
<p><h4>The Wobbly Table: Increased Social Presence via Subtle Incidental Movement of a Real-Virtual Table</h4>
IEEE VR proceedings<br />
<div class="authors">Myungho Lee, Kangsoo Kim, Salam Daher, Andrew Raij, Ryan Schubert, Jeremy Bailenson, Greg Welch</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0198-file3.jpg"></div><strong>Abstract: </strong>In this paper, we examined the effects of subtle incidental movement of a real-virtual table on presence and social presence. A between-subjects design was used, in which the table wobbled in &#8220;wobbly group&#8221; while the table was fixed in &#8220;control group.&#8221; In both groups, participants carried out a conversational task with a virtual human while seated at a table spanning a real-virtual environment. Our study showed the wobbly group exhibited a general increase in presence and social presence, with statistically significant increases in presence, co-presence, and attentional allocation. In addition, participants in the wobbly group showed more affective attraction for the virtual human.
</p>
<br />
<p><h4>Do Variations in Agency Indirectly Affect Behavior with Others? An Analysis of Gaze Behavior</h4>
TVCG<br />
<div class="authors">Andrew Robb, Andrea Kleinsmith, Andrew Cordar, Casey White, Samsun Lampotang, Adam Wendling, Benjamin Lok</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0127-file3.jpg"></div><strong>Abstract: </strong>We consider whether one teammate&#8217;s agency can indirectly affect behavior with other teammates, as well as directly with himself. To do so, we examined gaze behavior during a training exercise, in which sixty-nine nurses worked with two teammates. The agency of the two teammates were varied between conditions. Nurses&#8217; gaze behavior was coded using videos of their interactions. Agency was observed to directly affect behavior, such that participants spent more time gazing at virtual teammates than human teammates. However, participants continued to obey polite gaze norms with virtual teammates. In contrast, agency was not observed to indirectly affect gaze behavior.
</p>
<br />
<p><h4>MMSpace: Kinetically-augmented telepresence for small group-to-group conversations</h4>
IEEE VR proceedings<br />
<div class="authors">Kazuhiro Otsuka</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0138-file3.jpg"></div><strong>Abstract: </strong>A research prototype, MMSpace, was developed for realistic social telepresence in small group-to-group conversations. It consists of kinetic display avatars that can change the screen pose and position by automatically mirroring the remote user&#8217;s head motions. The kinetic avatars of MMSpace can produce highly accurate physical motions, by using 4-Degree-of-Freedom actuators. MMSpace supports eye contact between every pair of participants, by integrating multimodal visual attention cues. Subjective evaluations based on group discussions (2 x 2 setting) indicate that the kinetic display avatar is superior to static displays in gaze-awareness, eye-contact, perception of nonverbal behaviors, mutual understanding, and sense of telepresence.
</p>
<br />
<p><h4>Interactive and Adaptive Data-Driven Crowd Simulation</h4>
IEEE VR proceedings<br />
<div class="authors">Sujeong Kim, Aniket Bera, Andrew Best, Rohan Chabra, Dinesh Manocha</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0242-file3.jpg"></div><strong>Abstract: </strong>We present an adaptive data-driven algorithm for interactive crowd simulation. Our approach combines realistic trajectory behaviors extracted from videos with synthetic multi-agent algorithms to generate plausible simulations.
 We use statistical techniques to compute the movement patterns and motion dynamics from noisy 2D trajectories extracted from crowd videos.  These learned pedestrian dynamic characteristics are used to generate collision-free trajectories of virtual pedestrians in slightly different environments or situations. The overall approach is robust and can generate perceptually realistic crowd movements at interactive rates in dynamic environments.   We also present results from  preliminary user studies that evaluate the result of our algorithm.
</p>
<br />
<h3 class="catTitle" id="earsHands">3:30 pm &#8211; 5:00 pm<br />Ears and Hands</h3>
<strong>Session Chair: Sho Sakurai</strong>
<p><h4>Interactive Coupled Sound Synthesis-Propagation using Single Point Multipole Expansion</h4>
TVCG<br />
<div class="authors">Atul Rungta, Carl Schissler, Ravish Mehra, Chris Malloy, Ming Lin, Dinesh Manocha</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0137-file3.jpg"><strong>Abstract: </strong>Recent research in sound simulation has focused on either sound synthesis or sound propagation, and many standalone algorithms have been developed for each domain. 
We present a novel technique for coupling sound synthesis with sound propagation to automatically generate realistic aural content for virtual environments. 
Our approach can generate sounds from rigid-bodies based on the vibration modes and radiation coefficients represented by the single-point multipole expansion. We present a mode-adaptive propagation algorithm that uses a perceptual Hankel function approximation technique to achieve interactive runtime performance. 
The overall approach allows for high degrees of dynamism &#8211; it can support dynamic sources, dynamic listeners, and dynamic directivity simultaneously. 
We have integrated our system with the Unity game engine and demonstrate the effectiveness of this fully-automatic technique for audio content creation in complex indoor and outdoor scenes
</p>
<br />
<p><h4>Efficient HRTF-based Spatial Audio for Area and Volumetric Sources</h4>
TVCG<br />
<div class="authors">Carl Schissler, Aaron Nicholls, Ravish Mehra</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/217.jpg"></div><strong>Abstract: </strong>We present a novel spatial audio rendering technique to handle sound sources that can be represented by either an area or a volume in VR environments. As opposed to point-sampled sound sources, our approach projects the area-volumetric source to the spherical domain centered at the listener and represents this projection area compactly using the spherical harmonic (SH) basis functions. By representing the head-related transfer function (HRTF) in the same basis, we demonstrate that spatial audio which corresponds to an area-volumetric source can be efficiently computed as a dot product of the SH coefficients of the projection area and the HRTF. This results in an efficient technique whose computational complexity and memory requirements are independent of the complexity of the sound source. Our approach can support dynamic area-volumetric sound sources at interactive rates. We evaluate the performance of our technique in large complex VR environments and demonstrate significant improvement over the na¨ıve point-sampling technique. We also present results of a user evaluation, conducted to quantify the subjective preference of the user for our approach over the point-sampling approach in VR environments.

</p>
<br />
<p><h4>A Lightweight Electrotactile Feedback Device to Improve Grasping in Immersive Virtual Environments</h4>
IEEE VR proceedings<br />
<div class="authors">Johannes Hummel, Janki Dodiya, Laura Eckardt, Robin Wolff, Andreas Gerndt, Torsten Kuhlen</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0228-file3.jpg"></div><strong>Abstract: </strong>An immersive virtual environment is the ideal platform for the planning and training of on-orbit servicing missions. In such kind of virtual assembly simulation, grasping virtual objects is one of the most common and natural interactions.
In this paper, we present a novel, small and lightweight electrotactile feedback device, specifically designed for immersive virtual environments. We conducted a study to assess the feasibility and usability of our interaction device. Results show that electrotactile feedback improved the user&#8217;s grasping in our virtual on-orbit servicing scenario. The task completion time was significantly lower and the precision of the user&#8217;s interaction was higher.
</p>
<br />
<p><h4>Interaction with Virtual Object using Deformable Hand</h4>
IEEE VR proceedings<br />
<div class="authors">Koichi Hirota, Kazuyoshi Tagawa</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0250-file3.jpg"></div><strong>Abstract: </strong>This study investigated the implementation of a hand model and contact simulation method. The proposed method seeks hand form that minimizes the position and orientation errors on those areas. Deformation of the soft tissue of the hand was simulated by FEM, the friction of contact was introduced by the penalty method, and a model that is based on metaballs (or blobs) was employed to represent the smooth surface of the object. Through experimental implementation, it was proved that object manipulation such as pinching and grasping are possible and that the update rate of simulation can be approximately 50 Hz.
</p>
<br />
<h3 class="catTitle" id="cga">3:30 pm &#8211; 5:00 pm<br />IEEE Computer Graphics &#038; Applications</h3>
<p><h4>Spatial User Interfaces for Large-Scale Projector-Based Augmented Reality</h4>
<div class="authors">Michael R. Marner, Ross T. Smith, James A. Walsh, Bruce H. Thomas</div>
<strong>Abstract: </strong>Spatial augmented reality applies the concepts of spatial user interfaces to large-scale, projector-based augmented reality. Such virtual environments have interesting characteristics. They deal with large physical objects, the projection surfaces are nonplanar, the physical objects provide natural passive haptic feedback, and the systems naturally support collaboration between users. The article describes how these features affect the design of spatial user interfaces for these environments and explores promising research directions and application domains.
</p>
<br />
<p><h4>The Reality Deck&#8211;an Immersive Gigapixel Display</h4>
<div class="authors">Charilaos Papadopoulos, Kaloian Petkov, Member, Arie E. Kaufman, Klaus Mueller</div>
<strong>Abstract: </strong>The Reality Deck is a visualization facility offering state-of-the-art aggregate resolution and immersion. Its a 1.5-Gpixel immersive tiled display with a full 360-degree horizontal field of view. Comprising 416 high-density LED-backlit LCD displays, it visualizes gigapixel-resolution data while providing 20/20 visual acuity for most of the visualization space.
</p>
<br /><br />
<div id="tuesday">Tuesday, 22nd March 2016</div>
<h3 class="catTitle" id="system">8:30 am &#8211; 10:00 am<br />System and Latency</h3>
<strong>Session Chair: Eric Ragan</strong>
<p><h4>From Motion to Photons in 80 Microseconds: Towards Minimal Latency for Virtual and Augmented Reality</h4>
TVCG<br />
<div class="authors">Peter Lincoln, Alex Blate, Montek Singh, Turner Whitted, Andrei State, Anselmo Lastra, Henry Fuchs</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0182-file3.jpg"></div><strong>Abstract: </strong>We describe an AR, see-through, DMD display with an extremely fast (16kHz) update rate, combining post-rendering 2-D offsets and just-in-time tracking updates with a novel modulation technique for turning binary pixels into grayscale. These processing elements are mounted along with the optical display elements in a head-tracked rig where users view synthetic imagery superimposed on their real environment. Combining near-zero-latency mechanical tracking with FPGA display processing gives us a measured average of 80µs of end-to-end latency and a versatile test platform for extremely-low-latency display systems. We have examined the trade-offs between image quality and complexity and have maintained quality with this simple display modulation.
</p>
<br />
<br />
<p><h4>Construction and Evaluation of an Ultra Low Latency Frameless Renderer for VR</h4>
TVCG<br />
<div class="authors">Sebastian Friston, Anthony Steed, Simon Tilbury, Georgi Gaydadjiev</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0143-file3.jpg.png"><strong>Abstract: </strong>Latency is detrimental to VR. Typically considered to be discrete in time and space, in practice it changes across the display &#8211; and how it does so depends on the rendering approach used. We present an ultra-low latency realtime ray-caster: a frameless renderer with a distinct latency profile to that of a traditional frame-based system. We examine its performance when driving an Oculus DK2, comparing it with a GPU based system. We find that ours, with a lower latency, has higher fidelity under user motion, and that low display persistence reduces the sensitivity to velocity of both systems.
</p>
<br />
<p><h4>Estimating latency and concurrency of Asynchronous Real-Time Interactive Systems using Model Checking</h4>
IEEE VR proceedings<br />
<div class="authors">Stephan Rehfeld, Marc Erich Latoschik, Henrik Tramberend</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0305-file3.jpg"><strong>Abstract: </strong>This article introduces model checking as an alternative method to estimate the latency and parallelism of asynchronous Realtime Interactive Systems (RISs). Five typical concurrency and synchronization schemes often found in concurrent Virtual Reality systems are identified as use-cases. Several model-checking tools are evaluated against typical requirements in the RIS area. The formal language Rebeca and its model checker RMC are applied to the specification of the use-cases to estimate latency and parallelism for each case. The estimations are compared to measured results achieved by classical profiling.
</p>
<br />
<p><h4>Latency in Distributed Acquisition and Rendering for Telepresence Systems</h4>
TVCG Presentation<br />
<div class="authors">Stephan Ohl, Malte Willert, Oliver Staadt</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/ppt1.jpg"></div><strong>Abstract: </strong>—Telepresence systems use 3D techniques to create a more natural human-centered communication over long distances. This work concentrates on the analysis of latency in telepresence systems where acquisition and rendering are distributed. Keeping latency low is important to immerse users in the virtual environment. To better understand latency problems and to identify the source of such latency, we focus on the decomposition of system latency into sub-latencies. We contribute a model of latency and show how it can be used to estimate latencies in a complex telepresence dataflow network. To compare the estimates with real latencies in our prototype, we modify two common latency measurement methods. This presented methodology enables the developer to optimize the design, find implementation issues and gain deeper knowledge about specific sources of latency.
</p>
<br /><br />
<h3 class="catTitle" id="perception">10:15 am &#8211; 12:30 pm<br />Perception and Cognition</h3>
<strong>Session Chair: Tabitha Peck</strong>
<p><h4>Who turned the clock? Effects of Manipulated Zeitgebers, Cognitive Load and Immersion on Time Estimation</h4>
TVCG<br />
<div class="authors">Christian Schatzschneider, Gerd Bruder, Frank Steinicke</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0255-file3.jpg"></div><strong>Abstract: </strong>In this article we explore the effects of manipulated zeitgebers, cognitive load and immersion on time estimation as yet unexplored factors of spatiotemporal perception in virtual environments. We present an experiment in which we analyze human sensitivity to temporal durations with an HMD. We found that manipulations of external zeitgebers caused by a natural or unnatural movement of the virtual sun had a significant effect on time judgments. Moreover, using the dual-task paradigm the results show that increased spatial and verbal cognitive load resulted in a significant shortening of judged time as well as an interaction with the external zeitgebers.
</p>
<br />
<p><h4>Augmented Reality as a Countermeasure for Sleep Deprivation</h4>
TVCG<br />
<div class="authors">James Baumeister, Jillian Dorrian, Siobhan Banks, Alex Chatburn, Ross T. Smith, Mary A. Carskadon, Kurt Lushington, Bruce T. Thomas</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0207-file3.jpg"></div><strong>Abstract: </strong>Sleep deprivation is known to have serious deleterious effects on executive functioning and job performance. Augmented reality has an ability to place pertinent information at the fore, guiding visual focus and reducing instructional complexity. This paper presents a study to explore how spatial augmented reality instructions impact procedural task performance on sleep deprived users. The user study was conducted to examine performance on a procedural task at six time points over the course of a night of total sleep deprivation. Tasks were provided either by spatial augmented reality-based projections or on an adjacent monitor. The results indicate that participant errors significantly increased with the monitor condition when sleep deprived. The augmented reality condition exhibited a positive influence with participant errors and completion time having no significant increase when sleep deprived. The results of our study show that spatial augmented reality is an effective sleep deprivation countermeasure under laboratory conditions.
</p>
<br />
<p><h4>The Impact of a Self-Avatar on Cognitive Ability in Immersive Virtual Reality</h4>
IEEE VR proceedings<br />
<div class="authors">Anthony Steed, Ye Pan, Fiona Zisch, William Steptoe</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0225-file3.jpg"></div><strong>Abstract: </strong>We demonstrate that a self-avatar may aid the participant&#8217;s cognitive processes while immersed in a virtual reality system. Participants were asked to memorise pairs of letters, perform a spatial rotation exercise and then recall the pairs of letters. In a between-subject factor they either had an avatar or not, and in a within-subject factor they were instructed to keep their hands still or not. We found that participants who both had an avatar and were allowed to move their hands had significantly higher letter pair recall. There was no significant difference between the other three conditions.
</p>
<br />
<p><h4>An `In the Wild&#8217; Experiment on Presence and Embodiment using Consumer Virtual Reality Equipment</h4>
TVCG<br />
<div class="authors">Anthony Steed, Sebastian Friston, Maria Murcia Lopez, Jason Drummond, Ye Pan, David Swapp</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0227-file3.jpg"></div><strong>Abstract: </strong>We report on a study on presence and embodiment within virtual reality that was conducted `in the wild&#8217;. Users of Samsung Gear VR and Google Cardboard ran an app that presented a scenario where the participant would sit in a bar watching a singer. Despite the uncontrolled situation of the experiment, results from an in-app questionnaire showed tentative evidence that a self-avatar had a positive effect on self-report of presence and embodiment, and that having the singer invite the participant to tap along had a negative effect on self-report of embodiment.
</p>
<br />
<p><h4>Testing intuitive decision-making in VR: personality traits predict decisions in an accident situation</h4>
IEEE VR proceedings<br />
<div class="authors">UiJong Ju, June Kang, Christian Wallraven</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0108-file3.jpg"></div><strong>Abstract: </strong>Our study used virtual reality to investigate intuitive decision-making under time pressure. During an immersive racing game, participants were suddenly confronted with pedestrians appearing on the course. We observed three different reactions to this accident situation: group 1 did not brake, group 2 braked, and group 3 tried to avoid the pedestrians. Importantly, we found from personality surveys that the no-brake group had lower scores in perspective-taking and higher scores in psychopathy compared to the other groups. Our results demonstrate that personality differences are able to predict intuitive decision-making and that such processes can be studied in VR simulations.
</p>
<br />
<p><h4>Head tracking Latency in Virtual Environments Revisited: Do users with multiple sclerosis notice latency less?</h4>
TVCG Presentation<br />
<div class="authors">Gayani Samaraweera, Rongkai Guo, John Quarles</div>
<strong>Abstract: </strong>Latency (i.e., time delay) in a Virtual Environment is known to disrupt user performance, presence and induce simulator sickness. Thus, with emerging use of Virtual Rehabilitation, the target populations’ latency perception thresholds need to be considered to fully understand and possibly control the implications of latency in a Virtual Rehabilitation environment. We present a study that quantifies the latency discrimination thresholds of a yet untested population &#8211; a specific subset of mobility impaired participants where participants suffer from Multiple Sclerosis &#8211; and compare the results to a control group of healthy participants. The study was modeled after previous latency discrimination research and shows significant differences in latency perception between the two populations with MS participants showing lower sensitivity to latency than healthy participants.
</p>
<br /><br />
<h3 class="catTitle" id="arCalibration">1:45 pm &#8211; 3:15 pm<br />Augmented Reality and Calibration</h3>
<strong>Session Chair: Daisuke Iwai</strong>
<p><h4>Temporal Coherence Strategies for Augmented Reality Labeling</h4>
TVCG<br />
<div class="authors">Jacob Boesen Madsen, Markus Tatzgern, Claus B. Madsen, Dieter Schmalstieg, Denis Kalkofen</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0186-file3.jpg"></div><strong>Abstract: </strong>Temporal coherence of annotations is an important factor in augmented reality user interfaces and for information visualization. We empirically evaluate four different techniques for annotation. Based on these findings, we follow up with subjective evaluations in a second experiment. Results show that presenting annotations in object space or image space leads to a significant difference in task performance. Furthermore, there is a significant interaction between rendering space and update frequency of annotations. Participants improve significantly in locating annotations, when annotations are presented in object space, and view management update rate is limited. In a follow-up experiment, participants appear to be more satisfied with limited update rate in comparison to a continuous update rate of the view management system.
</p>
<br />
<p><h4>Adaptive Information Density for Augmented Reality Displays</h4>
IEEE VR proceedings<br />
<div class="authors">Markus Tatzgern, Valeria Orso, Denis Kalkofen, Giulio Jacucci, Luciano Gamberini, Dieter Schmalstieg</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0185-file3.jpg"></div><strong>Abstract: </strong>Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Users preferred our interface, because it provided a better overview of the data and allowed for easier comparison. We evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.
</p>
<br />
<p><h4>Evaluating Wide-Field-of-View Augmented Reality with Mixed Reality Simulation</h4>
IEEE VR proceedings<br />
<div class="authors">Donghao Ren, Tibor Goldschwendt, YunSuk Chang, Tobias Hollerer</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0118-file3.jpg"></div><strong>Abstract: </strong>Full-surround AR is an under explored-topic. Anticipating a change in AR capabilities, we experiment with wide-field-of-view annotations that link elements far apart in the visual field. We have built a system that simulates AR with different capacities. We conducted a study comparing user performance on five different task groups within an information-seeking scenario, comparing two different fields of view and presence and absence of tracking artifacts. A constrained field of view significantly increased task completion time. We found indications for task time effects of tracking artifacts to vary depending on age.
</p>
<br />
<p><h4>A Calibration Method for Optical See-through Head-mounted Displays with a Depth Camera</h4>
IEEE VR proceedings<br />
<div class="authors">Hanseul Jun, Gunhee Kim</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0163-file3.jpg"></div><strong>Abstract: </strong>We propose a fast and accurate calibration method for the optical see-through head-mounted displays (OST-HMD), taking advantage of affordable OST-HMDs with depth cameras that are widely appearing in the commercial market.
In order to correctly reflect the user experience into the calibration process, our method demands a user wearing the HMD to repeatedly point at rendered virtual circles with their fingertips.
From the repeated calibration data, we perform two stages of full calibration and simplified calibration.
Our experimental results show that the full and simplified calibration can be achieved with 10 and 5 user&#8217;s repetitions.
</p>
<br /><br />
<h3 class="catTitle" id="displays">3:30 pm &#8211; 5:00 pm<br />Displays and Sensory Integration</h3>
<strong>Session Chair: Gerd Bruder</strong>
<p><h4>Inter-reflection Compensation of Immersive Projection Display by Spatio-Temporal Screen Reflectance Modulation</h4>
TVCG<br />
<div class="authors">Shoichi Takeda, Daisuke Iwai, Kosuke Sato</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0248-file3.jpg"></div><strong>Abstract: </strong>We propose a novel inter-reflection compensation technique for immersive projection displays wherein we spatially modulate the reflectance pattern on the screen to improve the compensation performance of conventional methods. We realize spatial reflectance modulation by painting it with a photochromic compound, which changes its color when ultraviolet (UV) light is applied and by controlling UV irradiation with a UV LED array placed behind the screen. The main contribution is a computational model to optimize a reflectance pattern for the accurate reproduction of a target appearance by decreasing the inter-reflection effects. Through simulation and physical experiments, we demonstrate and confirm the proposal&#8217;s advantage over conventional methods.
</p>
<br />
<p><h4>Effects of Configuration of Optical Combiner on Near-Field Depth Perception in Optical See-Through Head-Mounted Displays</h4>
TVCG<br />
<div class="authors">Sangyoon Lee, Hong Hua</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0290-file3.jpg"></div><strong>Abstract: </strong>Ray-shift phenomenon means apparent distance shift in display image plane between virtual and physical objects. It is caused by difference in refraction of virtual display and see-through optical paths induced by optical combiners in optical see-through head-mounted displays. Through an experiment, we investigated effects of ray-shift phenomenon on depth perception for near-field distances in various configurations of optical combiner. According to experimental results, measured depth perception errors were similar to estimated ones, where |estimated percentage error| > 0.3%. Participants showed significantly larger depth perception errors in the horizontal-tilt configuration than in an ordinary condition, but not in the vertical-tilt configuration.
</p>
<br />
<p><h4>Gaze Prediction using Machine Learning for Dynamic Stereo Manipulation in Games</h4>
IEEE VR proceedings<br />
<div class="authors">George-Alex Koulieris, George Drettakis, Douglas Cunningham, Katerina Mania</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0159-file3.jpg"><strong>Abstract: </strong>Comfortable, high-quality 3D stereo viewing is becoming a requirement for interactive applications. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this around the object the user is gazing at. Player actions are highly correlated with the present state of a game, encoded by game variables. We train a classifier to learn these correlations 
using an eye-tracker. The classifier is used at runtime to predict gaze during game play. We use this prediction to propose a dynamic disparity manipulation method, providing rich and comfortable depth.
</p>
<br />
<p><h4>Motion Effects Synthesis for 4D Films</h4>
TVCG Presentation<br />
<div class="authors">Jaebong Lee, Bohyung Han, Seungmoon Choi</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/ppt3.jpg"></div><strong>Abstract: </strong>4D film is an immersive entertainment system that presents various physical effects with a film in order to enhance viewers’ experiences. Despite the recent emergence of 4D theaters, production of 4D effects relies on manual authoring. In this paper, we present algorithms that synthesize three classes of motion effects from the audiovisual content of a film. The first class of motion effects is those responding to fast camera motion to enhance the immersiveness of point-of-view shots, delivering fast and dynamic vestibular feedback. The second class moves viewers as closely as possible to the trajectory of slowly moving camera. Such motion provides an illusional effect of observing the scene from a distance while moving slowly within the scene. For these two classes, our algorithms compute the relative camera motion and then map it to a motion command to the 4D chair using appropriate motion mapping algorithms. The last class is for special effects, such as explosions, and our algorithm uses sound for the synthesis of impulses and vibrations. We assessed the subjective quality of our algorithms by user experiments, and results indicated that our algorithms can provide compelling motion effects.
</p>
<br /><br />
<div id="wednesday">Wednesday, 23rd March 2016</div>
<h3 class="catTitle" id="interaction">10:00 am &#8211; 12:30 pm<br />Interaction and Immersion</h3>
<strong>Session Chair: Alexander Kulik</strong>
<p><h4>Motion Capture with Ellipsoidal Skeleton using Multiple Depth Cameras</h4>
TVCG Presentation<br />
<div class="authors">Liang Shuai, Chao Li, Xiaohu Guo, Balakrishnan Prabhakaran, Jinxiang Chai</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/ppt4.jpg"></div><strong>Abstract: </strong>This paper introduces a novel motion capturing framework which works by minimizing the fitting error between an ellipsoid based skeleton and the input point cloud data captured by multiple depth cameras. The novelty of this method comes from that it uses the ellipsoids equipped with the spherical harmonics encoded displacement and normal functions to capture the geometry details of the tracked object. This method is also integrated with a mechanism to avoid collisions of bones during the motion capturing process. The method is implemented parallelly with CUDA on GPU and has a fast running speed without dedicated code optimization. The errors of the proposed method on the data from Berkeley Multimodal Human Action Database (MHAD) are within a reasonable range compared with the ground truth results. Our experiment shows that this method succeeds on many challenging motions which are failed to be reported by Microsoft Kinect SDK and not tested by existing works. In the comparison with the state-of-art marker-less depth camera based motion tracking work our method shows advantages in both robustness and input data modality.
</p>
<br />
<p><h4>Lift-Off: Using Reference Imagery and Freehand Sketching to Create 3D Models in VR</h4>
TVCG<br />
<div class="authors">Bret Jackson, Daniel Keefe</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0164-file3.jpg"></div><strong>Abstract: </strong>Current VR-based 3D modeling tools suffer from two problems that limit creativity and applicability: (1) the lack of control for freehand modeling, and (2) the difficulty of starting from scratch. To address these challenges, we present Lift-Off, an immersive 3D interface for creating complex models with a controlled, handcrafted style. Artists start with 2D sketches, which are then positioned in VR. 2D curves within the sketches are selected interactively and “lifted” into space to create a 3D scaffolding. Finally, artists sweep surfaces along these curves to create 3D models. Evaluations are presented for both long-term users and for novices.
</p>
<br />
<p><h4>Design and Evaluation of Data Annotation Workflows for CAVE-like Virtual Environments</h4>
TVCG<br />
<div class="authors">Sebastian Pick, Benjamin Weyers, Bernd Hentschel, Torsten W. Kuhlen</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0155-file3.jpg"><strong>Abstract: </strong>Data annotation is used in Virtual Reality applications to facilitate the data analysis process, e.g., architectural reviews. While many annotation systems have been presented, we observe that the process of metadata handling is often not covered thoroughly. To improve this situation, we present an annotation workflow that supports the creation, access, and modification of annotation contents in a flexible fashion. As an initial evaluation, we performed a user study in a CAVE-like virtual environment which compared our design to two alternatives in terms of an annotation creation task. Our design obtained good results in terms of task performance and user experience.
</p>
<br />
<p><h4>Examining Rotation Gain in CAVE-like Virtual Environments</h4>
TVCG<br />
<div class="authors">Sebastian Freitag, Benjamin Weyers, Torsten W. Kuhlen</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0184-file3.jpg"></div><strong>Abstract: </strong>In this work, we examined the effects of rotation gain in a CAVE-like virtual environment. The results show no significant effects of rotation gain within the range of [0.85;1.18] on simulator sickness, presence, or user performance in a cognitive task, but indicate that there is a negative influence on spatial knowledge for inexperienced users. Furthermore, we could show a negative correlation between simulator sickness and presence, cognitive performance and spatial knowledge, a positive correlation of presence and spatial knowledge, a mitigating influence of previous experience on simulator sickness, and a higher incidence of simulator sickness in women.
</p>
<br />
<p><h4>Visual Quality Adjustment for Volume Rendering in a Head-Tracked Virtual Environment</h4>
TVCG<br />
<div class="authors">Claudia Hänel, Benjamin Weyers, Bernd Hentschel, Torsten Kuhlen</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0188-file3.jpg"></div><strong>Abstract: </strong>In this work, we evaluate the trade-off between visual quality and latency in volume rendering applications for virtual environments. Therefore, we present results of a controlled user study. Search and count tasks were performed with varying volume rendering conditions applied according to viewer position updates. Our results indicate that participants preferred continuous adjustment of the visual quality over an instantaneous adjustment that guaranteed for low latency and over no adjustment providing constant high visual quality but rather low frame rates.
Within the continuous condition, participants showed best task performance and felt less disturbed by effects of the visualization during moving.
</p>
<br />
<p><h4>Extrafoveal Video Extension for an Immersive Viewing Experience</h4>
TVCG Presentation<br />
<div class="authors">Laura Turban, Fabrice Urban, Philippe Guillotel</div>
<strong>Abstract: </strong>Between the recent popularity of virtual reality (VR) and the development of 3D, immersion has become an integral part of entertainment concepts. Head-mounted Display (HMD) devices are often used to afford users a feeling of immersion in the environment. Another technique is to project additional material surrounding the viewer, as is achieved using cave systems. As a continuation of this technique, it could be interesting to extend surrounding projection to current television or cinema screens. The idea would be to entirely fill the viewer’s field of vision, thus providing them with a more complete feeling of being in the scene and part of the story. The appropriate content can be captured using large field of view (FoV) technology, using a rig of cameras for 110◦ to 360◦ capture, or created using computer generated images. The FoV is, however, rather limited in its use for existing (legacy) content, achieving between 36 to 90 degrees (◦) field, depending on the distance from the screen. This paper seeks to improve this FoV limitation by proposing computer vision techniques to extend such legacy content to the peripheral (extrafoveal) vision without changing the original creative intent or damaging the viewer’s experience. A new methodology is also proposed for performing user tests in order to evaluate the quality of the experience and confirm that the sense of immersion has been increased. This paper thus presents: i) an algorithm to spatially extend the video based on human vision characteristics, ii) its subjective results compared to state-of-the-art techniques, iii) the protocol required to evaluate the quality of the experience (QoE), and iv) the results of the user tests.
</p>
<br />
<p><h4>A Full Body Steerable Wind Display for a Locomotion Interface</h4>
TVCG Presentation<br />
<div class="authors">Sandip D. Kulkarni, Charles J. Fisher, Price Lefler, Aditya Desai, Shanthanu Chakravarthy, Eric R. Pardyjak, Mark A. Minor, John M. Hollerbach</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/ppt5.jpg"></div><strong>Abstract: </strong>—This paper presents the Treadport Active Wind Tunnel (TPAWT)-a full-body immersive virtual environment for the Treadport locomotion interface designed for generating wind on a user from any frontal direction at speeds up to 20 kph. The goal is to simulate the experience of realistic wind while walking in an outdoor virtual environment. A recirculating-type wind tunnel was created around the pre-existing Treadport installation by adding a large fan, ducting, and enclosure walls. Two sheets of air in a non-intrusive design flow along the side screens of the back-projection CAVE-like visual display, where they impinge and mix at the front screen to redirect towards the user in a full-body cross-section. By varying the flow conditions of the air sheets, the direction and speed of wind at the user are controlled. Design challenges to fit the wind tunnel in the pre-existing facility, and to manage turbulence to achieve stable and steerable flow, were overcome. The controller performance for wind speed and direction is demonstrated experimentally.
</p>
<br /><br />
<h3 class="catTitle" id="applications">1:45 pm &#8211; 3:15 pm<br />Applications</h3>
<strong>Session Chair: John Quarles</strong>
<p><h4>Taking Immersive VR Leap in Training of Landing Signal Officers</h4>
TVCG<br />
<div class="authors">Larry Greunke, Amela Sadagic</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0148-file3.jpg"></div><strong>Abstract: </strong>A major training device used to train all Landing Signal Officers (LSOs) has been two-stories tall simulator called Landing Signal Officer Trainer, Device 2H111. LSOs typically encounter this system for only six one-hour long sessions, leaving multiple gaps in training. This paper details our efforts on designing, buidling and testing a lightweight VR prototype training system that uses commercial off-the-shelf solutions, and provides LSOs with an unlimited number of training opportunities unrestricted by location and time. The results achieved in this effort indicate that the time for LSO training to make the leap to immersive VR has decidedly come.
</p>
<br /><br /><br />
<!-- <p><h4>Multimodal Adaptive Social Interaction in Virtual Environment (MASI-VR) for children with Autism Spectrum Disorders (ASD)</h4>
IEEE VR proceedings<br />
<div class="authors">Esubalew Bekele, Joshua Wade, Dayi Bian, Jing Fan, Zachary Warren, Nilanjan Sarkar</div>
<div class="paperTeaser"><img src="http://ieeevr.org/2016/wp-content/uploads/2016/02/VR2016-0174-file3.jpg"></div><strong>Abstract: </strong>Difficulties in social interaction, verbal and non-verbal communications as well as repetitive and atypical patterns of behavior, characterizes Autism spectrum disorders (ASD). A number of studies indicated that many children with ASD prefer technology and this preference can be explored to develop systems that may alleviate several challenges of traditional intervention. Recent advances in VR technology ushered in innovative assistive technologies for ASD intervention.  The current work presents design, development and a usability study of an adaptive multimodal virtual reality-based social interaction platform showing how eye gaze and task performance can be used in real-time to adapt intervention in VR.
</p>
<br /> -->
<p><h4>Programming Moves: Design and Evaluation of Applying Embodied Interaction in Virtual Environments to Enhance Computational Thinking in Middle School Students</h4>
IEEE VR proceedings<br />
<div class="authors">Dhaval Parmar, Joseph Isaac, Kara Gundersen, Nikeetha D&#8217;Souza, Alison Leonard, Sophie Jörg, Shaundra Daily, Sabarish V. Babu</div>
<img class="imageTeaser" src="../../wp-content/uploads/2016/02/VR2016-0215-file3.jpg.png"><strong>Abstract: </strong>We detail the design, implementation, and initial evaluation of a virtual reality education and entertainment application called Virtual Environment Interactions (VEnvI), in which students learn computer science concepts through the process of choreographing movement for a virtual character using a fun and intuitive interface. Participants programmatically crafted a dance performance for a virtual human and participated in an immersive embodied interaction metaphor in VEnvI. We qualitatively and quantitatively evaluated the extent to which the activities within VEnvI facilitated students’ edutainment, presence, interest, excitement, and engagement in computing, and the potential to alter their perceptions of computing and computer scientists.
</p>
<br />
<p><h4>Using a Virtual Environment to Study the Impact of Sending Traffic Alerts to Texting Pedestrians</h4>
IEEE VR proceedings<br />
<div class="authors">Pooya Rahimian, Elizabeth O&#8217;Neal, Junghum Paul Yon, Luke Franzen, Yuanyuan Jiang, Jodie Plumert, Joseph Kearney</div>
<div class="paperTeaser"><img src="../../wp-content/uploads/2016/02/VR2016-0181-file3.jpg"></div><strong>Abstract: </strong>This paper presents an experiment conducted in a large-screen virtual environment to evaluate how texting pedestrians respond to permissive traffic alerts delivered via their cell phone. We compared gap selection and movement timing in three conditions: texting, texting with alerts, and no texting (control). Participants in the control and alert groups chose larger gaps, were more discriminating in their gap choices, and had more time to spare than participants in the texting group. The alert group also paid the least attention to the roadway. The results demonstrate the potential and risks of Vehicle-to-Pedestrian (V2P) communications technology for mitigating pedestrian-vehicle crashes.
</p>
<br />																	</div>
									</article>
							
							</div>
														<footer class="site-info" itemscope itemtype="http://schema.org/WPFooter">
										© 2015-2016 IEEE Virtual Reality.
				</footer>
														</div>
					</div>
	</div>
</div>
        <div id="fb-root"></div>
		<script>(function(d, s, id) {
		  var js, fjs = d.getElementsByTagName(s)[0];
		  if (d.getElementById(id)) return;
		  js = d.createElement(s); js.id = id;
		  js.src = "../../../../connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3";
		  fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));</script>
		<!--facebook like and share js -->                   
        <!--<div id="fb-root"></div>
        <script>
        (function(d, s, id) {
          var js, fjs = d.getElementsByTagName(s)[0];
          if (d.getElementById(id)) return;
          js = d.createElement(s); js.id = id;
          js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&appId=1425108201100352&version=v2.0";
          fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));</script>-->
 	         <!--google share and  like and e js -->
        <script type="text/javascript">
            window.___gcfg = {
              lang: 'en-US'
            };
            (function() {
                var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
                po.src = '../../../../apis.google.com/js/plusone.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
            })();
        </script>
		<script type='text/javascript' src='../../../../apis.google.com/js/plusone.js'></script>
        <script type='text/javascript' src='../../../../apis.google.com/js/platform.js'></script>
        <!-- google share -->
        <script type="text/javascript">
          (function() {
            var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
            po.src = '../../../../apis.google.com/js/platform.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
          })();
        </script>
			<!-- twitter JS End -->
		<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="../../../../platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>	
	     <script>
	    jQuery( document ).scroll(function( $ )
		{
	    	var y = jQuery(this).scrollTop();
	      	if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent))
			{	 
			   if(jQuery(window).scrollTop() + jQuery(window).height() >= jQuery(document).height()-100)
			   {
				  jQuery('.sfsi_outr_div').css({'z-index':'9996',opacity:1,top:jQuery(window).scrollTop()+"px",position:"absolute"});
				  jQuery('.sfsi_outr_div').fadeIn(200);
				  jQuery('.sfsi_FrntInner_chg').fadeIn(200);
			   }
			   else{
				   jQuery('.sfsi_outr_div').fadeOut();
				   jQuery('.sfsi_FrntInner_chg').fadeOut();
			   }
		  }
		  else
		  {
			   if(jQuery(window).scrollTop() + jQuery(window).height() >= jQuery(document).height()-3)
			   {
					jQuery('.sfsi_outr_div').css({'z-index':'9996',opacity:1,top:jQuery(window).scrollTop()+200+"px",position:"absolute"});
					jQuery('.sfsi_outr_div').fadeIn(200);
					jQuery('.sfsi_FrntInner_chg').fadeIn(200);
		  	   }
	 		   else
			   {
				 jQuery('.sfsi_outr_div').fadeOut();
				 jQuery('.sfsi_FrntInner_chg').fadeOut();
			   }
	 	  } 
		});
     </script>
     <script src='../../wp-content/plugins/ultimate-social-media-icons/js/jquery-migrate-min3a05.js?ver=4.2.2'></script>
<script src='../../wp-content/plugins/ultimate-social-media-icons/js/jquery-ui-min3a05.js?ver=4.2.2'></script>
<script src='../../wp-content/plugins/ultimate-social-media-icons/js/shuffle/modernizr.custom.min3a05.js?ver=4.2.2'></script>
<script src='../../wp-content/plugins/ultimate-social-media-icons/js/shuffle/jquery.shuffle.min3a05.js?ver=4.2.2'></script>
<script src='../../wp-content/plugins/ultimate-social-media-icons/js/shuffle/random-shuffle-min3a05.js?ver=4.2.2'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ajax_object = {"ajax_url":"http:\/\/ieeevr.org\/2016\/wp-admin\/admin-ajax.php"};
var ajax_object = {"ajax_url":"http:\/\/ieeevr.org\/2016\/wp-admin\/admin-ajax.php","plugin_url":"http:\/\/ieeevr.org\/2016\/wp-content\/plugins\/ultimate-social-media-icons\/"};
/* ]]> */
</script>
<script src='../../wp-content/plugins/ultimate-social-media-icons/js/custom3a05.js?ver=4.2.2'></script>
</body>

<!-- Mirrored from ieeevr.org/2016/program/papers/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 22 May 2020 15:39:14 GMT -->
</html>